{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 5\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    numFilters = 128\n",
    "\n",
    "    filterSizes = [2, 3, 4, 5]\n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 1  # 二分类设置为1，多分类设置为类别的数目\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\ipykernel_launcher.py:129: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self.labelList = []\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        \n",
    "        if self.config.numClasses == 1:\n",
    "            labels = df[\"sentiment\"].tolist()\n",
    "        elif self.config.numClasses > 1:\n",
    "            labels = df[\"rate\"].tolist()\n",
    "            \n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "    \n",
    "    def _labelToIndex(self, labels, label2idx):\n",
    "        \"\"\"\n",
    "        将标签转换成索引表示\n",
    "        \"\"\"\n",
    "        labelIds = [label2idx[label] for label in labels]\n",
    "        return labelIds\n",
    "    \n",
    "    def _wordToIndex(self, reviews, word2idx):\n",
    "        \"\"\"\n",
    "        将词转换成索引\n",
    "        \"\"\"\n",
    "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
    "        return reviewIds\n",
    "        \n",
    "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        reviews = []\n",
    "        for review in x:\n",
    "            if len(review) >= self._sequenceLength:\n",
    "                reviews.append(review[:self._sequenceLength])\n",
    "            else:\n",
    "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(y[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(y[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews, labels):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        \n",
    "        uniqueLabel = list(set(labels))\n",
    "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
    "        self.labelList = list(range(len(uniqueLabel)))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(word2idx, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(label2idx, f)\n",
    "        \n",
    "        return word2idx, label2idx\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"PAD\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        word2idx, label2idx = self._genVocabulary(reviews, labels)\n",
    "        \n",
    "        # 将标签和句子数值化\n",
    "        labelIds = self._labelToIndex(labels, label2idx)\n",
    "        reviewIds = self._wordToIndex(reviews, word2idx)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviewIds, labelIds, word2idx, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000,)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            # 卷积的输入是思维[batch_size, width, height, channel]，因此需要增加维度，用tf.expand_dims来增大维度\n",
    "            self.embeddedWordsExpanded = tf.expand_dims(self.embeddedWords, -1)\n",
    "\n",
    "        # 创建卷积和池化层\n",
    "        pooledOutputs = []\n",
    "        # 有三种size的filter，3， 4， 5，textCNN是个多通道单层卷积的模型，可以看作三个单层的卷积模型的融合\n",
    "        for i, filterSize in enumerate(config.model.filterSizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filterSize):\n",
    "                # 卷积层，卷积核尺寸为filterSize * embeddingSize，卷积核的个数为numFilters\n",
    "                # 初始化权重矩阵和偏置\n",
    "                filterShape = [filterSize, config.model.embeddingSize, 1, config.model.numFilters]\n",
    "                W = tf.Variable(tf.truncated_normal(filterShape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[config.model.numFilters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embeddedWordsExpanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                \n",
    "                # relu函数的非线性映射\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                \n",
    "                # 池化层，最大池化，池化是对卷积后的序列取一个最大值\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, config.sequenceLength - filterSize + 1, 1, 1],  # ksize shape: [batch, height, width, channels]\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooledOutputs.append(pooled)  # 将三种size的filter的输出一起加入到列表中\n",
    "\n",
    "        # 得到CNN网络的输出长度\n",
    "        numFiltersTotal = config.model.numFilters * len(config.model.filterSizes)\n",
    "        \n",
    "        # 池化后的维度不变，按照最后的维度channel来concat\n",
    "        self.hPool = tf.concat(pooledOutputs, 3)\n",
    "        \n",
    "        # 摊平成二维的数据输入到全连接层\n",
    "        self.hPoolFlat = tf.reshape(self.hPool, [-1, numFiltersTotal])\n",
    "\n",
    "        # dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.hDrop = tf.nn.dropout(self.hPoolFlat, self.dropoutKeepProb)\n",
    "       \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[numFiltersTotal, config.numClasses],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.logits = tf.nn.xw_plus_b(self.hDrop, outputW, outputB, name=\"logits\")\n",
    "            if config.numClasses == 1:\n",
    "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.int32, name=\"predictions\")\n",
    "            elif config.numClasses > 1:\n",
    "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "            \n",
    "            print(self.predictions)\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
    "                                                                                                    dtype=tf.float32))\n",
    "            elif config.numClasses > 1:\n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
    "                \n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定义各类性能指标\n",
    "\"\"\"\n",
    "\n",
    "def mean(item: list) -> float:\n",
    "    \"\"\"\n",
    "    计算列表中元素的平均值\n",
    "    :param item: 列表对象\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def accuracy(pred_y, true_y):\n",
    "    \"\"\"\n",
    "    计算二类和多类的准确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "    corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == true_y[i]:\n",
    "            corr += 1\n",
    "    acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
    "    return acc\n",
    "\n",
    "\n",
    "def binary_precision(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的精确率计算\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    pred_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == positive:\n",
    "            pred_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    prec = corr / pred_corr if pred_corr > 0 else 0\n",
    "    return prec\n",
    "\n",
    "\n",
    "def binary_recall(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    true_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if true_y[i] == positive:\n",
    "            true_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    rec = corr / true_corr if true_corr > 0 else 0\n",
    "    return rec\n",
    "\n",
    "\n",
    "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
    "    \"\"\"\n",
    "    二类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param beta: beta值\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    precision = binary_precision(pred_y, true_y, positive)\n",
    "    recall = binary_recall(pred_y, true_y, positive)\n",
    "    try:\n",
    "        f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
    "    except:\n",
    "        f_b = 0\n",
    "    return f_b\n",
    "\n",
    "\n",
    "def multi_precision(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的精确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
    "    prec = mean(precisions)\n",
    "    return prec\n",
    "\n",
    "\n",
    "def multi_recall(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
    "    rec = mean(recalls)\n",
    "    return rec\n",
    "\n",
    "\n",
    "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
    "    \"\"\"\n",
    "    多类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :param beta: beta值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    f_betas = [binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
    "    f_beta = mean(f_betas)\n",
    "    return f_beta\n",
    "\n",
    "\n",
    "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到二分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = binary_recall(pred_y, true_y)\n",
    "    precision = binary_precision(pred_y, true_y)\n",
    "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
    "    return acc, recall, precision, f_beta\n",
    "\n",
    "\n",
    "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到多分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param labels:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = multi_recall(pred_y, true_y, labels)\n",
    "    precision = multi_precision(pred_y, true_y, labels)\n",
    "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
    "    return acc, recall, precision, f_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"output/predictions:0\", shape=(?, 1), dtype=int32)\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to E:\\githubProject\\textClassifier\\textCNN\\summarys\n",
      "\n",
      "start training model\n",
      "train: step: 1, loss: 1.7170578241348267, acc: 0.5625, recall: 0.7424242424242424, precision: 0.5568181818181818, f_beta: 0.6363636363636362\n",
      "train: step: 2, loss: 3.3845863342285156, acc: 0.4765625, recall: 0.0, precision: 0.0, f_beta: 0\n",
      "train: step: 3, loss: 1.8833599090576172, acc: 0.4609375, recall: 0.2153846153846154, precision: 0.4375, f_beta: 0.28865979381443296\n",
      "train: step: 4, loss: 1.7652866840362549, acc: 0.578125, recall: 0.8088235294117647, precision: 0.5729166666666666, f_beta: 0.6707317073170731\n",
      "train: step: 5, loss: 2.80256986618042, acc: 0.4609375, recall: 0.8333333333333334, precision: 0.45871559633027525, f_beta: 0.591715976331361\n",
      "train: step: 6, loss: 2.1209263801574707, acc: 0.5234375, recall: 0.8714285714285714, precision: 0.5398230088495575, f_beta: 0.6666666666666666\n",
      "train: step: 7, loss: 1.9711476564407349, acc: 0.453125, recall: 0.6363636363636364, precision: 0.4117647058823529, f_beta: 0.5\n",
      "train: step: 8, loss: 1.666806936264038, acc: 0.5546875, recall: 0.4745762711864407, precision: 0.5185185185185185, f_beta: 0.49557522123893805\n",
      "train: step: 9, loss: 2.4909117221832275, acc: 0.5, recall: 0.11666666666666667, precision: 0.3888888888888889, f_beta: 0.17948717948717952\n",
      "train: step: 10, loss: 2.856239080429077, acc: 0.4140625, recall: 0.08450704225352113, precision: 0.375, f_beta: 0.13793103448275862\n",
      "train: step: 11, loss: 2.164334535598755, acc: 0.5234375, recall: 0.07017543859649122, precision: 0.3333333333333333, f_beta: 0.11594202898550725\n",
      "train: step: 12, loss: 1.5129483938217163, acc: 0.5, recall: 0.31666666666666665, precision: 0.4523809523809524, f_beta: 0.37254901960784315\n",
      "train: step: 13, loss: 1.9850984811782837, acc: 0.4609375, recall: 0.5833333333333334, precision: 0.5185185185185185, f_beta: 0.5490196078431373\n",
      "train: step: 14, loss: 1.7313331365585327, acc: 0.5546875, recall: 0.8243243243243243, precision: 0.580952380952381, f_beta: 0.6815642458100559\n",
      "train: step: 15, loss: 1.9385120868682861, acc: 0.5, recall: 0.835820895522388, precision: 0.5137614678899083, f_beta: 0.6363636363636364\n",
      "train: step: 16, loss: 2.0957021713256836, acc: 0.4921875, recall: 0.8153846153846154, precision: 0.5, f_beta: 0.6198830409356725\n",
      "train: step: 17, loss: 1.534241795539856, acc: 0.578125, recall: 0.746268656716418, precision: 0.5747126436781609, f_beta: 0.6493506493506493\n",
      "train: step: 18, loss: 1.2700532674789429, acc: 0.5625, recall: 0.7413793103448276, precision: 0.5119047619047619, f_beta: 0.6056338028169014\n",
      "train: step: 19, loss: 1.4064877033233643, acc: 0.4921875, recall: 0.4533333333333333, precision: 0.5862068965517241, f_beta: 0.5112781954887218\n",
      "train: step: 20, loss: 1.6114757061004639, acc: 0.5234375, recall: 0.2786885245901639, precision: 0.5, f_beta: 0.35789473684210527\n",
      "train: step: 21, loss: 1.7432750463485718, acc: 0.546875, recall: 0.26666666666666666, precision: 0.5333333333333333, f_beta: 0.3555555555555555\n",
      "train: step: 22, loss: 1.6105258464813232, acc: 0.5234375, recall: 0.2, precision: 0.48, f_beta: 0.28235294117647064\n",
      "train: step: 23, loss: 1.85673987865448, acc: 0.5, recall: 0.34328358208955223, precision: 0.5348837209302325, f_beta: 0.41818181818181815\n",
      "train: step: 24, loss: 1.634968876838684, acc: 0.4765625, recall: 0.3728813559322034, precision: 0.4230769230769231, f_beta: 0.39639639639639646\n",
      "train: step: 25, loss: 1.4741401672363281, acc: 0.515625, recall: 0.6, precision: 0.4864864864864865, f_beta: 0.5373134328358209\n",
      "train: step: 26, loss: 1.3508797883987427, acc: 0.53125, recall: 0.640625, precision: 0.5256410256410257, f_beta: 0.5774647887323944\n",
      "train: step: 27, loss: 1.7004752159118652, acc: 0.5234375, recall: 0.7818181818181819, precision: 0.4673913043478261, f_beta: 0.5850340136054423\n",
      "train: step: 28, loss: 1.5077176094055176, acc: 0.5234375, recall: 0.7377049180327869, precision: 0.5, f_beta: 0.5960264900662251\n",
      "train: step: 29, loss: 1.356327772140503, acc: 0.5625, recall: 0.6610169491525424, precision: 0.52, f_beta: 0.582089552238806\n",
      "train: step: 30, loss: 1.3489629030227661, acc: 0.5703125, recall: 0.5079365079365079, precision: 0.5714285714285714, f_beta: 0.5378151260504201\n",
      "train: step: 31, loss: 1.2670971155166626, acc: 0.5859375, recall: 0.38461538461538464, precision: 0.6578947368421053, f_beta: 0.4854368932038836\n",
      "train: step: 32, loss: 1.5121681690216064, acc: 0.5625, recall: 0.24561403508771928, precision: 0.5185185185185185, f_beta: 0.3333333333333333\n",
      "train: step: 33, loss: 1.6490739583969116, acc: 0.5234375, recall: 0.2923076923076923, precision: 0.5588235294117647, f_beta: 0.3838383838383838\n",
      "train: step: 34, loss: 1.137399435043335, acc: 0.5625, recall: 0.46153846153846156, precision: 0.46153846153846156, f_beta: 0.46153846153846156\n",
      "train: step: 35, loss: 1.2823125123977661, acc: 0.5546875, recall: 0.5806451612903226, precision: 0.5373134328358209, f_beta: 0.5581395348837209\n",
      "train: step: 36, loss: 1.1405045986175537, acc: 0.5078125, recall: 0.5857142857142857, precision: 0.5466666666666666, f_beta: 0.5655172413793104\n",
      "train: step: 37, loss: 1.2974624633789062, acc: 0.5390625, recall: 0.7101449275362319, precision: 0.5568181818181818, f_beta: 0.6242038216560509\n",
      "train: step: 38, loss: 1.2133938074111938, acc: 0.546875, recall: 0.7068965517241379, precision: 0.5, f_beta: 0.5857142857142857\n",
      "train: step: 39, loss: 1.0610945224761963, acc: 0.5546875, recall: 0.7457627118644068, precision: 0.5116279069767442, f_beta: 0.6068965517241379\n",
      "train: step: 40, loss: 1.1691172122955322, acc: 0.515625, recall: 0.559322033898305, precision: 0.4782608695652174, f_beta: 0.515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 41, loss: 1.2354586124420166, acc: 0.53125, recall: 0.41025641025641024, precision: 0.6956521739130435, f_beta: 0.5161290322580646\n",
      "train: step: 42, loss: 1.018622636795044, acc: 0.5390625, recall: 0.4375, precision: 0.5490196078431373, f_beta: 0.48695652173913045\n",
      "train: step: 43, loss: 1.2427058219909668, acc: 0.4609375, recall: 0.4084507042253521, precision: 0.5178571428571429, f_beta: 0.4566929133858268\n",
      "train: step: 44, loss: 1.0436692237854004, acc: 0.5859375, recall: 0.4918032786885246, precision: 0.5769230769230769, f_beta: 0.5309734513274337\n",
      "train: step: 45, loss: 1.111107349395752, acc: 0.578125, recall: 0.5, precision: 0.5370370370370371, f_beta: 0.5178571428571428\n",
      "train: step: 46, loss: 1.0695902109146118, acc: 0.6015625, recall: 0.6438356164383562, precision: 0.6527777777777778, f_beta: 0.6482758620689656\n",
      "train: step: 47, loss: 1.243180751800537, acc: 0.546875, recall: 0.6610169491525424, precision: 0.5064935064935064, f_beta: 0.5735294117647058\n",
      "train: step: 48, loss: 1.0479927062988281, acc: 0.578125, recall: 0.7142857142857143, precision: 0.5555555555555556, f_beta: 0.6250000000000001\n",
      "train: step: 49, loss: 1.1382986307144165, acc: 0.5546875, recall: 0.6883116883116883, precision: 0.6162790697674418, f_beta: 0.6503067484662576\n",
      "train: step: 50, loss: 0.9856632351875305, acc: 0.546875, recall: 0.5588235294117647, precision: 0.5757575757575758, f_beta: 0.5671641791044776\n",
      "train: step: 51, loss: 1.0540900230407715, acc: 0.5703125, recall: 0.6029411764705882, precision: 0.5942028985507246, f_beta: 0.5985401459854013\n",
      "train: step: 52, loss: 1.1274092197418213, acc: 0.53125, recall: 0.5, precision: 0.5666666666666667, f_beta: 0.53125\n",
      "train: step: 53, loss: 0.9004679918289185, acc: 0.578125, recall: 0.4423076923076923, precision: 0.4791666666666667, f_beta: 0.45999999999999996\n",
      "train: step: 54, loss: 0.9681706428527832, acc: 0.6484375, recall: 0.5909090909090909, precision: 0.6842105263157895, f_beta: 0.6341463414634148\n",
      "train: step: 55, loss: 0.894832968711853, acc: 0.6328125, recall: 0.5660377358490566, precision: 0.5555555555555556, f_beta: 0.5607476635514018\n",
      "train: step: 56, loss: 1.3097825050354004, acc: 0.484375, recall: 0.45901639344262296, precision: 0.45901639344262296, f_beta: 0.45901639344262296\n",
      "train: step: 57, loss: 0.8950374126434326, acc: 0.609375, recall: 0.7096774193548387, precision: 0.5789473684210527, f_beta: 0.6376811594202899\n",
      "train: step: 58, loss: 0.8919435739517212, acc: 0.5703125, recall: 0.6197183098591549, precision: 0.6111111111111112, f_beta: 0.6153846153846154\n",
      "train: step: 59, loss: 1.000631332397461, acc: 0.5703125, recall: 0.609375, precision: 0.5652173913043478, f_beta: 0.5864661654135338\n",
      "train: step: 60, loss: 0.9973992109298706, acc: 0.5703125, recall: 0.4931506849315068, precision: 0.6666666666666666, f_beta: 0.5669291338582677\n",
      "train: step: 61, loss: 0.9273946285247803, acc: 0.5703125, recall: 0.5084745762711864, precision: 0.5357142857142857, f_beta: 0.5217391304347826\n",
      "train: step: 62, loss: 0.836046576499939, acc: 0.6015625, recall: 0.7, precision: 0.620253164556962, f_beta: 0.6577181208053692\n",
      "train: step: 63, loss: 0.7953280806541443, acc: 0.6484375, recall: 0.6984126984126984, precision: 0.6285714285714286, f_beta: 0.6616541353383458\n",
      "train: step: 64, loss: 0.8316880464553833, acc: 0.609375, recall: 0.6615384615384615, precision: 0.6056338028169014, f_beta: 0.6323529411764706\n",
      "train: step: 65, loss: 0.7854331731796265, acc: 0.6328125, recall: 0.6865671641791045, precision: 0.6388888888888888, f_beta: 0.6618705035971223\n",
      "train: step: 66, loss: 1.0404689311981201, acc: 0.515625, recall: 0.5072463768115942, precision: 0.5555555555555556, f_beta: 0.5303030303030303\n",
      "train: step: 67, loss: 0.827187180519104, acc: 0.65625, recall: 0.6714285714285714, precision: 0.6911764705882353, f_beta: 0.681159420289855\n",
      "train: step: 68, loss: 0.7738852500915527, acc: 0.640625, recall: 0.6764705882352942, precision: 0.6571428571428571, f_beta: 0.6666666666666666\n",
      "train: step: 69, loss: 0.8884091377258301, acc: 0.6328125, recall: 0.5789473684210527, precision: 0.5892857142857143, f_beta: 0.584070796460177\n",
      "train: step: 70, loss: 0.9913927316665649, acc: 0.546875, recall: 0.5172413793103449, precision: 0.5, f_beta: 0.5084745762711865\n",
      "train: step: 71, loss: 0.7064909934997559, acc: 0.6796875, recall: 0.5901639344262295, precision: 0.6923076923076923, f_beta: 0.6371681415929203\n",
      "train: step: 72, loss: 0.7486066818237305, acc: 0.65625, recall: 0.5964912280701754, precision: 0.6181818181818182, f_beta: 0.607142857142857\n",
      "train: step: 73, loss: 0.8155012130737305, acc: 0.609375, recall: 0.5573770491803278, precision: 0.5964912280701754, f_beta: 0.576271186440678\n",
      "train: step: 74, loss: 0.7537546157836914, acc: 0.6484375, recall: 0.5645161290322581, precision: 0.660377358490566, f_beta: 0.6086956521739131\n",
      "train: step: 75, loss: 0.7796312570571899, acc: 0.65625, recall: 0.625, precision: 0.603448275862069, f_beta: 0.6140350877192983\n",
      "train: step: 76, loss: 0.6452489495277405, acc: 0.7109375, recall: 0.711864406779661, precision: 0.6774193548387096, f_beta: 0.6942148760330579\n",
      "train: step: 77, loss: 0.7623482942581177, acc: 0.6015625, recall: 0.7419354838709677, precision: 0.5679012345679012, f_beta: 0.6433566433566433\n",
      "train: step: 78, loss: 0.7567605972290039, acc: 0.625, recall: 0.7413793103448276, precision: 0.5657894736842105, f_beta: 0.6417910447761195\n",
      "train: step: 79, loss: 0.6763515472412109, acc: 0.6796875, recall: 0.5806451612903226, precision: 0.7058823529411765, f_beta: 0.6371681415929203\n",
      "train: step: 80, loss: 0.7799426317214966, acc: 0.6171875, recall: 0.5230769230769231, precision: 0.6538461538461539, f_beta: 0.5811965811965812\n",
      "train: step: 81, loss: 0.756978452205658, acc: 0.625, recall: 0.4918032786885246, precision: 0.6382978723404256, f_beta: 0.5555555555555556\n",
      "train: step: 82, loss: 0.7366384267807007, acc: 0.6328125, recall: 0.5526315789473685, precision: 0.7636363636363637, f_beta: 0.6412213740458015\n",
      "train: step: 83, loss: 0.7390795946121216, acc: 0.671875, recall: 0.6986301369863014, precision: 0.7183098591549296, f_beta: 0.7083333333333335\n",
      "train: step: 84, loss: 0.8371797800064087, acc: 0.6015625, recall: 0.7230769230769231, precision: 0.5875, f_beta: 0.6482758620689656\n",
      "train: step: 85, loss: 0.8042139410972595, acc: 0.625, recall: 0.8028169014084507, precision: 0.6263736263736264, f_beta: 0.7037037037037038\n",
      "train: step: 86, loss: 0.8510796427726746, acc: 0.625, recall: 0.9016393442622951, precision: 0.5670103092783505, f_beta: 0.6962025316455697\n",
      "train: step: 87, loss: 0.8043739795684814, acc: 0.6484375, recall: 0.8064516129032258, precision: 0.6024096385542169, f_beta: 0.689655172413793\n",
      "train: step: 88, loss: 0.7740436792373657, acc: 0.6484375, recall: 0.6567164179104478, precision: 0.6666666666666666, f_beta: 0.6616541353383459\n",
      "train: step: 89, loss: 0.9186460971832275, acc: 0.5625, recall: 0.35384615384615387, precision: 0.6216216216216216, f_beta: 0.4509803921568628\n",
      "train: step: 90, loss: 0.898455023765564, acc: 0.6015625, recall: 0.4126984126984127, precision: 0.65, f_beta: 0.5048543689320388\n",
      "train: step: 91, loss: 0.8252401947975159, acc: 0.59375, recall: 0.5070422535211268, precision: 0.6792452830188679, f_beta: 0.5806451612903225\n",
      "train: step: 92, loss: 0.7059676647186279, acc: 0.671875, recall: 0.5892857142857143, precision: 0.6346153846153846, f_beta: 0.611111111111111\n",
      "train: step: 93, loss: 0.6582871079444885, acc: 0.6796875, recall: 0.6935483870967742, precision: 0.6615384615384615, f_beta: 0.6771653543307087\n",
      "train: step: 94, loss: 0.7393385171890259, acc: 0.671875, recall: 0.8461538461538461, precision: 0.5641025641025641, f_beta: 0.676923076923077\n",
      "train: step: 95, loss: 0.6287006139755249, acc: 0.6640625, recall: 0.7636363636363637, precision: 0.5833333333333334, f_beta: 0.6614173228346456\n",
      "train: step: 96, loss: 0.6323370933532715, acc: 0.7265625, recall: 0.7164179104477612, precision: 0.75, f_beta: 0.7328244274809161\n",
      "train: step: 97, loss: 0.5193362236022949, acc: 0.734375, recall: 0.6774193548387096, precision: 0.75, f_beta: 0.7118644067796611\n",
      "train: step: 98, loss: 0.7837686538696289, acc: 0.59375, recall: 0.5423728813559322, precision: 0.5614035087719298, f_beta: 0.5517241379310345\n",
      "train: step: 99, loss: 0.5958826541900635, acc: 0.7421875, recall: 0.7428571428571429, precision: 0.7761194029850746, f_beta: 0.759124087591241\n",
      "train: step: 100, loss: 0.6635491847991943, acc: 0.6875, recall: 0.6290322580645161, precision: 0.6964285714285714, f_beta: 0.6610169491525423\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-16T18:34:24.364752, step: 100, loss: 0.46120075690440643, acc: 0.7878605769230769,precision: 0.7291171798161348, recall: 0.8303184098245436, f_beta: 0.7752756056290917\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-100\n",
      "\n",
      "train: step: 101, loss: 0.7218993902206421, acc: 0.640625, recall: 0.5666666666666667, precision: 0.6296296296296297, f_beta: 0.5964912280701755\n",
      "train: step: 102, loss: 0.6046165823936462, acc: 0.7265625, recall: 0.7741935483870968, precision: 0.6956521739130435, f_beta: 0.732824427480916\n",
      "train: step: 103, loss: 0.6617442965507507, acc: 0.65625, recall: 0.6142857142857143, precision: 0.7166666666666667, f_beta: 0.6615384615384615\n",
      "train: step: 104, loss: 0.6184207797050476, acc: 0.71875, recall: 0.7567567567567568, precision: 0.7567567567567568, f_beta: 0.7567567567567567\n",
      "train: step: 105, loss: 0.6317914724349976, acc: 0.6953125, recall: 0.8235294117647058, precision: 0.6746987951807228, f_beta: 0.7417218543046358\n",
      "train: step: 106, loss: 0.6683565378189087, acc: 0.6796875, recall: 0.7794117647058824, precision: 0.6708860759493671, f_beta: 0.7210884353741497\n",
      "train: step: 107, loss: 0.6940723657608032, acc: 0.625, recall: 0.72, precision: 0.5142857142857142, f_beta: 0.6\n",
      "train: step: 108, loss: 0.7804886102676392, acc: 0.59375, recall: 0.65, precision: 0.5571428571428572, f_beta: 0.6\n",
      "train: step: 109, loss: 0.5815012454986572, acc: 0.734375, recall: 0.7575757575757576, precision: 0.7352941176470589, f_beta: 0.746268656716418\n",
      "train: step: 110, loss: 0.6414827108383179, acc: 0.6953125, recall: 0.5588235294117647, precision: 0.8085106382978723, f_beta: 0.6608695652173914\n",
      "train: step: 111, loss: 0.670687198638916, acc: 0.671875, recall: 0.5294117647058824, precision: 0.782608695652174, f_beta: 0.631578947368421\n",
      "train: step: 112, loss: 0.49089378118515015, acc: 0.71875, recall: 0.6935483870967742, precision: 0.7166666666666667, f_beta: 0.7049180327868854\n",
      "train: step: 113, loss: 0.5836277008056641, acc: 0.703125, recall: 0.6875, precision: 0.7096774193548387, f_beta: 0.6984126984126984\n",
      "train: step: 114, loss: 0.526573657989502, acc: 0.734375, recall: 0.8615384615384616, precision: 0.691358024691358, f_beta: 0.7671232876712328\n",
      "train: step: 115, loss: 0.5723164081573486, acc: 0.75, recall: 0.8243243243243243, precision: 0.7625, f_beta: 0.7922077922077922\n",
      "train: step: 116, loss: 0.5935326814651489, acc: 0.671875, recall: 0.7903225806451613, precision: 0.6282051282051282, f_beta: 0.7000000000000001\n",
      "train: step: 117, loss: 0.5921568870544434, acc: 0.7109375, recall: 0.875, precision: 0.620253164556962, f_beta: 0.725925925925926\n",
      "train: step: 118, loss: 0.542939305305481, acc: 0.71875, recall: 0.7580645161290323, precision: 0.6911764705882353, f_beta: 0.7230769230769231\n",
      "train: step: 119, loss: 0.584721565246582, acc: 0.703125, recall: 0.6271186440677966, precision: 0.6981132075471698, f_beta: 0.6607142857142857\n",
      "train: step: 120, loss: 0.6765460968017578, acc: 0.671875, recall: 0.6176470588235294, precision: 0.7241379310344828, f_beta: 0.6666666666666667\n",
      "train: step: 121, loss: 0.6056072115898132, acc: 0.734375, recall: 0.5909090909090909, precision: 0.8478260869565217, f_beta: 0.6964285714285714\n",
      "train: step: 122, loss: 0.7006373405456543, acc: 0.6484375, recall: 0.515625, precision: 0.7021276595744681, f_beta: 0.5945945945945946\n",
      "train: step: 123, loss: 0.5945985317230225, acc: 0.6953125, recall: 0.6615384615384615, precision: 0.7166666666666667, f_beta: 0.688\n",
      "train: step: 124, loss: 0.42871004343032837, acc: 0.7890625, recall: 0.8333333333333334, precision: 0.746268656716418, f_beta: 0.7874015748031497\n",
      "train: step: 125, loss: 0.4985964894294739, acc: 0.765625, recall: 0.8142857142857143, precision: 0.7702702702702703, f_beta: 0.7916666666666666\n",
      "train: step: 126, loss: 0.5427215099334717, acc: 0.78125, recall: 0.9142857142857143, precision: 0.7441860465116279, f_beta: 0.8205128205128205\n",
      "train: step: 127, loss: 0.5803824067115784, acc: 0.7421875, recall: 0.8166666666666667, precision: 0.6901408450704225, f_beta: 0.748091603053435\n",
      "train: step: 128, loss: 0.52234947681427, acc: 0.7578125, recall: 0.8769230769230769, precision: 0.7125, f_beta: 0.7862068965517242\n",
      "train: step: 129, loss: 0.5504339933395386, acc: 0.703125, recall: 0.7213114754098361, precision: 0.676923076923077, f_beta: 0.6984126984126984\n",
      "train: step: 130, loss: 0.5057401061058044, acc: 0.7421875, recall: 0.7, precision: 0.7368421052631579, f_beta: 0.717948717948718\n",
      "train: step: 131, loss: 0.5846922397613525, acc: 0.703125, recall: 0.582089552238806, precision: 0.7959183673469388, f_beta: 0.6724137931034483\n",
      "train: step: 132, loss: 0.5733954906463623, acc: 0.75, recall: 0.6428571428571429, precision: 0.8653846153846154, f_beta: 0.7377049180327869\n",
      "train: step: 133, loss: 0.4883047938346863, acc: 0.7890625, recall: 0.6724137931034483, precision: 0.8297872340425532, f_beta: 0.7428571428571429\n",
      "train: step: 134, loss: 0.5942037105560303, acc: 0.7109375, recall: 0.696969696969697, precision: 0.7301587301587301, f_beta: 0.7131782945736433\n",
      "train: step: 135, loss: 0.46842968463897705, acc: 0.7578125, recall: 0.7285714285714285, precision: 0.8095238095238095, f_beta: 0.7669172932330828\n",
      "train: step: 136, loss: 0.5693427324295044, acc: 0.6953125, recall: 0.8243243243243243, precision: 0.7011494252873564, f_beta: 0.7577639751552796\n",
      "train: step: 137, loss: 0.661181628704071, acc: 0.6796875, recall: 0.8833333333333333, precision: 0.6091954022988506, f_beta: 0.7210884353741497\n",
      "train: step: 138, loss: 0.5227352976799011, acc: 0.7578125, recall: 0.8243243243243243, precision: 0.7721518987341772, f_beta: 0.7973856209150328\n",
      "train: step: 139, loss: 0.4675799608230591, acc: 0.78125, recall: 0.8082191780821918, precision: 0.8082191780821918, f_beta: 0.8082191780821918\n",
      "train: step: 140, loss: 0.5247693061828613, acc: 0.7578125, recall: 0.8059701492537313, precision: 0.75, f_beta: 0.776978417266187\n",
      "train: step: 141, loss: 0.5892237424850464, acc: 0.71875, recall: 0.7692307692307693, precision: 0.704225352112676, f_beta: 0.7352941176470589\n",
      "train: step: 142, loss: 0.5048881769180298, acc: 0.7734375, recall: 0.8095238095238095, precision: 0.75, f_beta: 0.7786259541984734\n",
      "train: step: 143, loss: 0.48745840787887573, acc: 0.7578125, recall: 0.75, precision: 0.7619047619047619, f_beta: 0.7559055118110236\n",
      "train: step: 144, loss: 0.5558896064758301, acc: 0.7578125, recall: 0.7301587301587301, precision: 0.7666666666666667, f_beta: 0.7479674796747968\n",
      "train: step: 145, loss: 0.5162819623947144, acc: 0.78125, recall: 0.6885245901639344, precision: 0.8235294117647058, f_beta: 0.7499999999999999\n",
      "train: step: 146, loss: 0.4564935863018036, acc: 0.7890625, recall: 0.7666666666666667, precision: 0.7796610169491526, f_beta: 0.773109243697479\n",
      "train: step: 147, loss: 0.5527480244636536, acc: 0.703125, recall: 0.6617647058823529, precision: 0.75, f_beta: 0.7031250000000001\n",
      "train: step: 148, loss: 0.461595356464386, acc: 0.78125, recall: 0.7540983606557377, precision: 0.7796610169491526, f_beta: 0.7666666666666666\n",
      "train: step: 149, loss: 0.5014193058013916, acc: 0.75, recall: 0.7575757575757576, precision: 0.7575757575757576, f_beta: 0.7575757575757576\n",
      "train: step: 150, loss: 0.5239988565444946, acc: 0.7421875, recall: 0.8840579710144928, precision: 0.7093023255813954, f_beta: 0.7870967741935485\n",
      "train: step: 151, loss: 0.49304163455963135, acc: 0.75, recall: 0.7903225806451613, precision: 0.7205882352941176, f_beta: 0.7538461538461538\n",
      "train: step: 152, loss: 0.4709344506263733, acc: 0.765625, recall: 0.835820895522388, precision: 0.7466666666666667, f_beta: 0.7887323943661971\n",
      "train: step: 153, loss: 0.4491739869117737, acc: 0.796875, recall: 0.8333333333333334, precision: 0.7575757575757576, f_beta: 0.7936507936507938\n",
      "train: step: 154, loss: 0.33202555775642395, acc: 0.859375, recall: 0.8392857142857143, precision: 0.8392857142857143, f_beta: 0.8392857142857143\n",
      "train: step: 155, loss: 0.38769423961639404, acc: 0.796875, recall: 0.7592592592592593, precision: 0.7592592592592593, f_beta: 0.7592592592592593\n",
      "train: step: 156, loss: 0.4761238396167755, acc: 0.7734375, recall: 0.6727272727272727, precision: 0.7708333333333334, f_beta: 0.7184466019417476\n",
      "start training model\n",
      "train: step: 157, loss: 0.5551187992095947, acc: 0.75, recall: 0.6721311475409836, precision: 0.7735849056603774, f_beta: 0.719298245614035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 158, loss: 0.4188133776187897, acc: 0.8203125, recall: 0.7377049180327869, precision: 0.8653846153846154, f_beta: 0.7964601769911505\n",
      "train: step: 159, loss: 0.5403763055801392, acc: 0.71875, recall: 0.6567164179104478, precision: 0.7719298245614035, f_beta: 0.7096774193548386\n",
      "train: step: 160, loss: 0.3765304982662201, acc: 0.7890625, recall: 0.7538461538461538, precision: 0.8166666666666667, f_beta: 0.784\n",
      "train: step: 161, loss: 0.5195302963256836, acc: 0.7421875, recall: 0.7794117647058824, precision: 0.7464788732394366, f_beta: 0.762589928057554\n",
      "train: step: 162, loss: 0.49647045135498047, acc: 0.7890625, recall: 0.9180327868852459, precision: 0.717948717948718, f_beta: 0.8057553956834532\n",
      "train: step: 163, loss: 0.47949525713920593, acc: 0.734375, recall: 0.8285714285714286, precision: 0.725, f_beta: 0.7733333333333333\n",
      "train: step: 164, loss: 0.4405886232852936, acc: 0.8203125, recall: 0.8727272727272727, precision: 0.75, f_beta: 0.8067226890756303\n",
      "train: step: 165, loss: 0.4383769631385803, acc: 0.78125, recall: 0.8235294117647058, precision: 0.7777777777777778, f_beta: 0.7999999999999999\n",
      "train: step: 166, loss: 0.36844807863235474, acc: 0.828125, recall: 0.7419354838709677, precision: 0.8846153846153846, f_beta: 0.8070175438596492\n",
      "train: step: 167, loss: 0.42622455954551697, acc: 0.828125, recall: 0.855072463768116, precision: 0.8309859154929577, f_beta: 0.8428571428571429\n",
      "train: step: 168, loss: 0.4041978716850281, acc: 0.828125, recall: 0.863013698630137, precision: 0.84, f_beta: 0.8513513513513513\n",
      "train: step: 169, loss: 0.41436830163002014, acc: 0.7734375, recall: 0.7945205479452054, precision: 0.8055555555555556, f_beta: 0.8\n",
      "train: step: 170, loss: 0.5059139728546143, acc: 0.7578125, recall: 0.7540983606557377, precision: 0.7419354838709677, f_beta: 0.7479674796747967\n",
      "train: step: 171, loss: 0.34409987926483154, acc: 0.84375, recall: 0.8769230769230769, precision: 0.8260869565217391, f_beta: 0.8507462686567164\n",
      "train: step: 172, loss: 0.49429571628570557, acc: 0.765625, recall: 0.7384615384615385, precision: 0.7868852459016393, f_beta: 0.7619047619047619\n",
      "train: step: 173, loss: 0.42229506373405457, acc: 0.8203125, recall: 0.7678571428571429, precision: 0.8113207547169812, f_beta: 0.7889908256880735\n",
      "train: step: 174, loss: 0.3738418221473694, acc: 0.8125, recall: 0.8490566037735849, precision: 0.7377049180327869, f_beta: 0.7894736842105263\n",
      "train: step: 175, loss: 0.40440264344215393, acc: 0.8359375, recall: 0.7833333333333333, precision: 0.8545454545454545, f_beta: 0.817391304347826\n",
      "train: step: 176, loss: 0.39592409133911133, acc: 0.8125, recall: 0.859375, precision: 0.7857142857142857, f_beta: 0.8208955223880597\n",
      "train: step: 177, loss: 0.3152998685836792, acc: 0.8984375, recall: 0.9253731343283582, precision: 0.8857142857142857, f_beta: 0.9051094890510949\n",
      "train: step: 178, loss: 0.4993016719818115, acc: 0.78125, recall: 0.7466666666666667, precision: 0.8615384615384616, f_beta: 0.8\n",
      "train: step: 179, loss: 0.2903628945350647, acc: 0.8984375, recall: 0.868421052631579, precision: 0.9565217391304348, f_beta: 0.9103448275862069\n",
      "train: step: 180, loss: 0.3921251893043518, acc: 0.84375, recall: 0.8153846153846154, precision: 0.8688524590163934, f_beta: 0.8412698412698412\n",
      "train: step: 181, loss: 0.43200552463531494, acc: 0.8125, recall: 0.9104477611940298, precision: 0.7721518987341772, f_beta: 0.8356164383561644\n",
      "train: step: 182, loss: 0.559302806854248, acc: 0.75, recall: 0.8421052631578947, precision: 0.676056338028169, f_beta: 0.75\n",
      "train: step: 183, loss: 0.4488665759563446, acc: 0.7734375, recall: 0.8596491228070176, precision: 0.7, f_beta: 0.7716535433070866\n",
      "train: step: 184, loss: 0.3912552297115326, acc: 0.8359375, recall: 0.896551724137931, precision: 0.7761194029850746, f_beta: 0.8320000000000001\n",
      "train: step: 185, loss: 0.39166566729545593, acc: 0.8203125, recall: 0.8571428571428571, precision: 0.7941176470588235, f_beta: 0.8244274809160305\n",
      "train: step: 186, loss: 0.4918483793735504, acc: 0.7890625, recall: 0.7543859649122807, precision: 0.7678571428571429, f_beta: 0.7610619469026548\n",
      "train: step: 187, loss: 0.3459206819534302, acc: 0.84375, recall: 0.7627118644067796, precision: 0.8823529411764706, f_beta: 0.8181818181818181\n",
      "train: step: 188, loss: 0.4569580554962158, acc: 0.765625, recall: 0.6666666666666666, precision: 0.8461538461538461, f_beta: 0.7457627118644068\n",
      "train: step: 189, loss: 0.5645185708999634, acc: 0.7421875, recall: 0.5671641791044776, precision: 0.9047619047619048, f_beta: 0.6972477064220184\n",
      "train: step: 190, loss: 0.5073565244674683, acc: 0.765625, recall: 0.8153846153846154, precision: 0.7464788732394366, f_beta: 0.7794117647058824\n",
      "train: step: 191, loss: 0.4092334806919098, acc: 0.8046875, recall: 0.8307692307692308, precision: 0.7941176470588235, f_beta: 0.8120300751879699\n",
      "train: step: 192, loss: 0.3890141248703003, acc: 0.8125, recall: 0.8923076923076924, precision: 0.7733333333333333, f_beta: 0.8285714285714286\n",
      "train: step: 193, loss: 0.3804674446582794, acc: 0.859375, recall: 0.9375, precision: 0.8108108108108109, f_beta: 0.8695652173913043\n",
      "train: step: 194, loss: 0.5879834294319153, acc: 0.6953125, recall: 0.8571428571428571, precision: 0.6428571428571429, f_beta: 0.7346938775510204\n",
      "train: step: 195, loss: 0.48550963401794434, acc: 0.8125, recall: 0.890625, precision: 0.7702702702702703, f_beta: 0.8260869565217391\n",
      "train: step: 196, loss: 0.44679075479507446, acc: 0.7578125, recall: 0.7796610169491526, precision: 0.71875, f_beta: 0.7479674796747967\n",
      "train: step: 197, loss: 0.3906540274620056, acc: 0.84375, recall: 0.8666666666666667, precision: 0.8125, f_beta: 0.8387096774193549\n",
      "train: step: 198, loss: 0.4570712447166443, acc: 0.7578125, recall: 0.7671232876712328, precision: 0.8, f_beta: 0.7832167832167832\n",
      "train: step: 199, loss: 0.40067753195762634, acc: 0.8046875, recall: 0.7241379310344828, precision: 0.8235294117647058, f_beta: 0.7706422018348623\n",
      "train: step: 200, loss: 0.5290024876594543, acc: 0.71875, recall: 0.5806451612903226, precision: 0.782608695652174, f_beta: 0.6666666666666667\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:34:43.747752, step: 200, loss: 0.4156966484509982, acc: 0.8125,precision: 0.7085089232610127, recall: 0.9017861004944461, f_beta: 0.7917612328695233\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-200\n",
      "\n",
      "train: step: 201, loss: 0.3718835711479187, acc: 0.8359375, recall: 0.7321428571428571, precision: 0.8723404255319149, f_beta: 0.7961165048543688\n",
      "train: step: 202, loss: 0.38205260038375854, acc: 0.8203125, recall: 0.7586206896551724, precision: 0.8301886792452831, f_beta: 0.7927927927927928\n",
      "train: step: 203, loss: 0.3634245693683624, acc: 0.8125, recall: 0.8095238095238095, precision: 0.8095238095238095, f_beta: 0.8095238095238095\n",
      "train: step: 204, loss: 0.42649534344673157, acc: 0.75, recall: 0.8214285714285714, precision: 0.6764705882352942, f_beta: 0.7419354838709677\n",
      "train: step: 205, loss: 0.5131859183311462, acc: 0.7578125, recall: 0.828125, precision: 0.726027397260274, f_beta: 0.7737226277372263\n",
      "train: step: 206, loss: 0.4318538308143616, acc: 0.78125, recall: 0.847457627118644, precision: 0.7246376811594203, f_beta: 0.7812499999999999\n",
      "train: step: 207, loss: 0.42501842975616455, acc: 0.8203125, recall: 0.796875, precision: 0.8360655737704918, f_beta: 0.816\n",
      "train: step: 208, loss: 0.3392840623855591, acc: 0.8515625, recall: 0.9253731343283582, precision: 0.8157894736842105, f_beta: 0.8671328671328671\n",
      "train: step: 209, loss: 0.3338993787765503, acc: 0.8125, recall: 0.8113207547169812, precision: 0.7543859649122807, f_beta: 0.7818181818181817\n",
      "train: step: 210, loss: 0.43085458874702454, acc: 0.8125, recall: 0.7666666666666667, precision: 0.8214285714285714, f_beta: 0.793103448275862\n",
      "train: step: 211, loss: 0.3678913712501526, acc: 0.84375, recall: 0.7777777777777778, precision: 0.8909090909090909, f_beta: 0.8305084745762712\n",
      "train: step: 212, loss: 0.44622623920440674, acc: 0.796875, recall: 0.75, precision: 0.7777777777777778, f_beta: 0.7636363636363638\n",
      "train: step: 213, loss: 0.43614521622657776, acc: 0.8046875, recall: 0.7868852459016393, precision: 0.8, f_beta: 0.7933884297520661\n",
      "train: step: 214, loss: 0.30441099405288696, acc: 0.859375, recall: 0.796875, precision: 0.9107142857142857, f_beta: 0.85\n",
      "train: step: 215, loss: 0.45080769062042236, acc: 0.765625, recall: 0.8387096774193549, precision: 0.7222222222222222, f_beta: 0.7761194029850746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 216, loss: 0.30870211124420166, acc: 0.890625, recall: 0.9, precision: 0.9, f_beta: 0.9\n",
      "train: step: 217, loss: 0.44526463747024536, acc: 0.796875, recall: 0.8059701492537313, precision: 0.8059701492537313, f_beta: 0.8059701492537313\n",
      "train: step: 218, loss: 0.37223169207572937, acc: 0.8515625, recall: 0.8571428571428571, precision: 0.8695652173913043, f_beta: 0.8633093525179856\n",
      "train: step: 219, loss: 0.327870637178421, acc: 0.8515625, recall: 0.8709677419354839, precision: 0.8307692307692308, f_beta: 0.8503937007874016\n",
      "train: step: 220, loss: 0.3479558527469635, acc: 0.859375, recall: 0.9264705882352942, precision: 0.8289473684210527, f_beta: 0.875\n",
      "train: step: 221, loss: 0.3556827902793884, acc: 0.8203125, recall: 0.8461538461538461, precision: 0.8088235294117647, f_beta: 0.8270676691729324\n",
      "train: step: 222, loss: 0.4267459213733673, acc: 0.796875, recall: 0.8333333333333334, precision: 0.7857142857142857, f_beta: 0.8088235294117647\n",
      "train: step: 223, loss: 0.3669314682483673, acc: 0.84375, recall: 0.8225806451612904, precision: 0.85, f_beta: 0.8360655737704918\n",
      "train: step: 224, loss: 0.3735271692276001, acc: 0.8203125, recall: 0.8387096774193549, precision: 0.8, f_beta: 0.8188976377952757\n",
      "train: step: 225, loss: 0.3119295537471771, acc: 0.859375, recall: 0.9076923076923077, precision: 0.8309859154929577, f_beta: 0.8676470588235293\n",
      "train: step: 226, loss: 0.3628859221935272, acc: 0.84375, recall: 0.8392857142857143, precision: 0.8103448275862069, f_beta: 0.8245614035087718\n",
      "train: step: 227, loss: 0.35578039288520813, acc: 0.828125, recall: 0.8548387096774194, precision: 0.803030303030303, f_beta: 0.828125\n",
      "train: step: 228, loss: 0.4617895483970642, acc: 0.7734375, recall: 0.78125, precision: 0.7692307692307693, f_beta: 0.7751937984496126\n",
      "train: step: 229, loss: 0.38649070262908936, acc: 0.859375, recall: 0.875, precision: 0.875, f_beta: 0.875\n",
      "train: step: 230, loss: 0.40276312828063965, acc: 0.8125, recall: 0.7571428571428571, precision: 0.8833333333333333, f_beta: 0.8153846153846154\n",
      "train: step: 231, loss: 0.40781623125076294, acc: 0.8359375, recall: 0.8305084745762712, precision: 0.8166666666666667, f_beta: 0.8235294117647058\n",
      "train: step: 232, loss: 0.43662771582603455, acc: 0.859375, recall: 0.847457627118644, precision: 0.847457627118644, f_beta: 0.847457627118644\n",
      "train: step: 233, loss: 0.40669137239456177, acc: 0.8359375, recall: 0.9206349206349206, precision: 0.7837837837837838, f_beta: 0.8467153284671532\n",
      "train: step: 234, loss: 0.3550759553909302, acc: 0.875, recall: 0.9032258064516129, precision: 0.8484848484848485, f_beta: 0.875\n",
      "train: step: 235, loss: 0.3012961745262146, acc: 0.8515625, recall: 0.8793103448275862, precision: 0.8095238095238095, f_beta: 0.8429752066115702\n",
      "train: step: 236, loss: 0.45936357975006104, acc: 0.7890625, recall: 0.7424242424242424, precision: 0.8305084745762712, f_beta: 0.784\n",
      "train: step: 237, loss: 0.41825398802757263, acc: 0.828125, recall: 0.7931034482758621, precision: 0.8214285714285714, f_beta: 0.8070175438596492\n",
      "train: step: 238, loss: 0.4242323637008667, acc: 0.8203125, recall: 0.746268656716418, precision: 0.8928571428571429, f_beta: 0.8130081300813009\n",
      "train: step: 239, loss: 0.45467695593833923, acc: 0.8046875, recall: 0.835820895522388, precision: 0.8, f_beta: 0.8175182481751825\n",
      "train: step: 240, loss: 0.35755449533462524, acc: 0.8203125, recall: 0.7857142857142857, precision: 0.873015873015873, f_beta: 0.8270676691729324\n",
      "train: step: 241, loss: 0.4444243311882019, acc: 0.7734375, recall: 0.819672131147541, precision: 0.7352941176470589, f_beta: 0.7751937984496124\n",
      "train: step: 242, loss: 0.4475289285182953, acc: 0.796875, recall: 0.9242424242424242, precision: 0.7439024390243902, f_beta: 0.8243243243243243\n",
      "train: step: 243, loss: 0.351482093334198, acc: 0.8203125, recall: 0.8771929824561403, precision: 0.7575757575757576, f_beta: 0.8130081300813008\n",
      "train: step: 244, loss: 0.3800504803657532, acc: 0.796875, recall: 0.8412698412698413, precision: 0.7681159420289855, f_beta: 0.8030303030303031\n",
      "train: step: 245, loss: 0.36203861236572266, acc: 0.8515625, recall: 0.8571428571428571, precision: 0.84375, f_beta: 0.8503937007874015\n",
      "train: step: 246, loss: 0.43883687257766724, acc: 0.8046875, recall: 0.7321428571428571, precision: 0.803921568627451, f_beta: 0.766355140186916\n",
      "train: step: 247, loss: 0.4447842836380005, acc: 0.8125, recall: 0.7313432835820896, precision: 0.8909090909090909, f_beta: 0.8032786885245902\n",
      "train: step: 248, loss: 0.39448535442352295, acc: 0.7734375, recall: 0.6875, precision: 0.8301886792452831, f_beta: 0.7521367521367521\n",
      "train: step: 249, loss: 0.3401145339012146, acc: 0.828125, recall: 0.9074074074074074, precision: 0.7424242424242424, f_beta: 0.8166666666666667\n",
      "train: step: 250, loss: 0.4791308641433716, acc: 0.765625, recall: 0.7580645161290323, precision: 0.7580645161290323, f_beta: 0.7580645161290323\n",
      "train: step: 251, loss: 0.3906669616699219, acc: 0.8203125, recall: 0.8103448275862069, precision: 0.7966101694915254, f_beta: 0.8034188034188032\n",
      "train: step: 252, loss: 0.3671882152557373, acc: 0.8515625, recall: 0.8970588235294118, precision: 0.8356164383561644, f_beta: 0.8652482269503545\n",
      "train: step: 253, loss: 0.3957952857017517, acc: 0.8515625, recall: 0.8070175438596491, precision: 0.8518518518518519, f_beta: 0.8288288288288288\n",
      "train: step: 254, loss: 0.4156896471977234, acc: 0.84375, recall: 0.8769230769230769, precision: 0.8260869565217391, f_beta: 0.8507462686567164\n",
      "train: step: 255, loss: 0.39801114797592163, acc: 0.8046875, recall: 0.8769230769230769, precision: 0.7702702702702703, f_beta: 0.8201438848920863\n",
      "train: step: 256, loss: 0.4868857264518738, acc: 0.8203125, recall: 0.8611111111111112, precision: 0.8266666666666667, f_beta: 0.8435374149659863\n",
      "train: step: 257, loss: 0.3931291401386261, acc: 0.828125, recall: 0.8596491228070176, precision: 0.7777777777777778, f_beta: 0.8166666666666668\n",
      "train: step: 258, loss: 0.33983176946640015, acc: 0.875, recall: 0.847457627118644, precision: 0.8771929824561403, f_beta: 0.8620689655172413\n",
      "train: step: 259, loss: 0.321459025144577, acc: 0.90625, recall: 0.8548387096774194, precision: 0.9464285714285714, f_beta: 0.8983050847457628\n",
      "train: step: 260, loss: 0.47275322675704956, acc: 0.859375, recall: 0.8082191780821918, precision: 0.9365079365079365, f_beta: 0.8676470588235294\n",
      "train: step: 261, loss: 0.4505464732646942, acc: 0.78125, recall: 0.8108108108108109, precision: 0.8108108108108109, f_beta: 0.8108108108108109\n",
      "train: step: 262, loss: 0.3754495680332184, acc: 0.8515625, recall: 0.875, precision: 0.8032786885245902, f_beta: 0.8376068376068376\n",
      "train: step: 263, loss: 0.3577001094818115, acc: 0.859375, recall: 0.8939393939393939, precision: 0.8428571428571429, f_beta: 0.8676470588235294\n",
      "train: step: 264, loss: 0.38442885875701904, acc: 0.8203125, recall: 0.8571428571428571, precision: 0.7941176470588235, f_beta: 0.8244274809160305\n",
      "train: step: 265, loss: 0.37033718824386597, acc: 0.859375, recall: 0.9298245614035088, precision: 0.7910447761194029, f_beta: 0.8548387096774193\n",
      "train: step: 266, loss: 0.34501397609710693, acc: 0.84375, recall: 0.8285714285714286, precision: 0.8787878787878788, f_beta: 0.8529411764705883\n",
      "train: step: 267, loss: 0.29761677980422974, acc: 0.8984375, recall: 0.8787878787878788, precision: 0.9206349206349206, f_beta: 0.8992248062015504\n",
      "train: step: 268, loss: 0.3859504163265228, acc: 0.8203125, recall: 0.8591549295774648, precision: 0.8243243243243243, f_beta: 0.8413793103448276\n",
      "train: step: 269, loss: 0.41716793179512024, acc: 0.828125, recall: 0.8676470588235294, precision: 0.8194444444444444, f_beta: 0.8428571428571429\n",
      "train: step: 270, loss: 0.4433276653289795, acc: 0.7578125, recall: 0.7666666666666667, precision: 0.7301587301587301, f_beta: 0.7479674796747968\n",
      "train: step: 271, loss: 0.36386799812316895, acc: 0.84375, recall: 0.8428571428571429, precision: 0.8676470588235294, f_beta: 0.855072463768116\n",
      "train: step: 272, loss: 0.31210941076278687, acc: 0.8515625, recall: 0.8412698412698413, precision: 0.8548387096774194, f_beta: 0.848\n",
      "train: step: 273, loss: 0.26562464237213135, acc: 0.890625, recall: 0.92, precision: 0.8961038961038961, f_beta: 0.9078947368421053\n",
      "train: step: 274, loss: 0.4617886543273926, acc: 0.8359375, recall: 0.9130434782608695, precision: 0.8076923076923077, f_beta: 0.8571428571428572\n",
      "train: step: 275, loss: 0.3656649887561798, acc: 0.828125, recall: 0.8823529411764706, precision: 0.8108108108108109, f_beta: 0.8450704225352113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 276, loss: 0.3372655510902405, acc: 0.84375, recall: 0.8125, precision: 0.8666666666666667, f_beta: 0.8387096774193549\n",
      "train: step: 277, loss: 0.24444182217121124, acc: 0.890625, recall: 0.8450704225352113, precision: 0.9523809523809523, f_beta: 0.8955223880597014\n",
      "train: step: 278, loss: 0.37984538078308105, acc: 0.84375, recall: 0.8571428571428571, precision: 0.8571428571428571, f_beta: 0.8571428571428571\n",
      "train: step: 279, loss: 0.3309183418750763, acc: 0.8359375, recall: 0.7936507936507936, precision: 0.8620689655172413, f_beta: 0.8264462809917354\n",
      "train: step: 280, loss: 0.3118654191493988, acc: 0.890625, recall: 0.9206349206349206, precision: 0.8656716417910447, f_beta: 0.8923076923076922\n",
      "train: step: 281, loss: 0.4167097806930542, acc: 0.7890625, recall: 0.8253968253968254, precision: 0.7647058823529411, f_beta: 0.7938931297709922\n",
      "train: step: 282, loss: 0.34054648876190186, acc: 0.8671875, recall: 0.9420289855072463, precision: 0.8333333333333334, f_beta: 0.8843537414965987\n",
      "train: step: 283, loss: 0.3646409511566162, acc: 0.8203125, recall: 0.8285714285714286, precision: 0.8405797101449275, f_beta: 0.8345323741007195\n",
      "train: step: 284, loss: 0.4103665351867676, acc: 0.84375, recall: 0.8524590163934426, precision: 0.8253968253968254, f_beta: 0.8387096774193549\n",
      "train: step: 285, loss: 0.35437458753585815, acc: 0.8359375, recall: 0.828125, precision: 0.8412698412698413, f_beta: 0.8346456692913385\n",
      "train: step: 286, loss: 0.3794615864753723, acc: 0.828125, recall: 0.8360655737704918, precision: 0.8095238095238095, f_beta: 0.8225806451612904\n",
      "train: step: 287, loss: 0.3264678716659546, acc: 0.84375, recall: 0.8307692307692308, precision: 0.8571428571428571, f_beta: 0.84375\n",
      "train: step: 288, loss: 0.3592380881309509, acc: 0.8515625, recall: 0.8387096774193549, precision: 0.8524590163934426, f_beta: 0.8455284552845529\n",
      "train: step: 289, loss: 0.3679739236831665, acc: 0.8359375, recall: 0.8870967741935484, precision: 0.7971014492753623, f_beta: 0.8396946564885496\n",
      "train: step: 290, loss: 0.3736361265182495, acc: 0.8359375, recall: 0.8461538461538461, precision: 0.8333333333333334, f_beta: 0.8396946564885497\n",
      "train: step: 291, loss: 0.37395939230918884, acc: 0.796875, recall: 0.7796610169491526, precision: 0.7796610169491526, f_beta: 0.7796610169491526\n",
      "train: step: 292, loss: 0.3389711081981659, acc: 0.8515625, recall: 0.8888888888888888, precision: 0.8235294117647058, f_beta: 0.8549618320610687\n",
      "train: step: 293, loss: 0.2935865819454193, acc: 0.859375, recall: 0.8513513513513513, precision: 0.9, f_beta: 0.875\n",
      "train: step: 294, loss: 0.3576459288597107, acc: 0.828125, recall: 0.7866666666666666, precision: 0.9076923076923077, f_beta: 0.8428571428571429\n",
      "train: step: 295, loss: 0.2904506325721741, acc: 0.8515625, recall: 0.8787878787878788, precision: 0.8405797101449275, f_beta: 0.8592592592592593\n",
      "train: step: 296, loss: 0.3701067566871643, acc: 0.8515625, recall: 0.9193548387096774, precision: 0.8028169014084507, f_beta: 0.8571428571428572\n",
      "train: step: 297, loss: 0.4466606080532074, acc: 0.78125, recall: 0.859375, precision: 0.7432432432432432, f_beta: 0.7971014492753623\n",
      "train: step: 298, loss: 0.2829231321811676, acc: 0.8671875, recall: 0.9259259259259259, precision: 0.7936507936507936, f_beta: 0.8547008547008547\n",
      "train: step: 299, loss: 0.35975727438926697, acc: 0.859375, recall: 0.9259259259259259, precision: 0.78125, f_beta: 0.847457627118644\n",
      "train: step: 300, loss: 0.31070107221603394, acc: 0.8359375, recall: 0.7586206896551724, precision: 0.8627450980392157, f_beta: 0.8073394495412843\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:35:03.155178, step: 300, loss: 0.3657750624876756, acc: 0.8449519230769231,precision: 0.7861672594755373, recall: 0.8957526703135937, f_beta: 0.835918953545173\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-300\n",
      "\n",
      "train: step: 301, loss: 0.43208712339401245, acc: 0.8125, recall: 0.75, precision: 0.8333333333333334, f_beta: 0.7894736842105262\n",
      "train: step: 302, loss: 0.37204235792160034, acc: 0.84375, recall: 0.7681159420289855, precision: 0.9298245614035088, f_beta: 0.8412698412698412\n",
      "train: step: 303, loss: 0.33485355973243713, acc: 0.84375, recall: 0.7068965517241379, precision: 0.9318181818181818, f_beta: 0.803921568627451\n",
      "train: step: 304, loss: 0.3911847472190857, acc: 0.859375, recall: 0.8108108108108109, precision: 0.9375, f_beta: 0.8695652173913043\n",
      "train: step: 305, loss: 0.316312700510025, acc: 0.84375, recall: 0.9104477611940298, precision: 0.8133333333333334, f_beta: 0.8591549295774649\n",
      "train: step: 306, loss: 0.34068411588668823, acc: 0.84375, recall: 0.9066666666666666, precision: 0.8395061728395061, f_beta: 0.8717948717948718\n",
      "train: step: 307, loss: 0.4191579520702362, acc: 0.8203125, recall: 0.9230769230769231, precision: 0.7692307692307693, f_beta: 0.8391608391608392\n",
      "train: step: 308, loss: 0.46811848878860474, acc: 0.7890625, recall: 0.9545454545454546, precision: 0.7241379310344828, f_beta: 0.823529411764706\n",
      "train: step: 309, loss: 0.38125845789909363, acc: 0.8359375, recall: 0.9242424242424242, precision: 0.7922077922077922, f_beta: 0.8531468531468532\n",
      "train: step: 310, loss: 0.33883199095726013, acc: 0.828125, recall: 0.9117647058823529, precision: 0.7948717948717948, f_beta: 0.8493150684931507\n",
      "train: step: 311, loss: 0.28282293677330017, acc: 0.890625, recall: 0.8688524590163934, precision: 0.8983050847457628, f_beta: 0.8833333333333333\n",
      "train: step: 312, loss: 0.25807151198387146, acc: 0.90625, recall: 0.8771929824561403, precision: 0.9090909090909091, f_beta: 0.8928571428571428\n",
      "start training model\n",
      "train: step: 313, loss: 0.3721146583557129, acc: 0.828125, recall: 0.7647058823529411, precision: 0.896551724137931, f_beta: 0.8253968253968255\n",
      "train: step: 314, loss: 0.36854735016822815, acc: 0.8203125, recall: 0.703125, precision: 0.9183673469387755, f_beta: 0.7964601769911506\n",
      "train: step: 315, loss: 0.3131219446659088, acc: 0.875, recall: 0.8064516129032258, precision: 0.9259259259259259, f_beta: 0.8620689655172414\n",
      "train: step: 316, loss: 0.263007253408432, acc: 0.875, recall: 0.8620689655172413, precision: 0.8620689655172413, f_beta: 0.8620689655172413\n",
      "train: step: 317, loss: 0.32784074544906616, acc: 0.8671875, recall: 0.9076923076923077, precision: 0.8428571428571429, f_beta: 0.8740740740740741\n",
      "train: step: 318, loss: 0.2986018657684326, acc: 0.875, recall: 0.8923076923076924, precision: 0.8656716417910447, f_beta: 0.8787878787878788\n",
      "train: step: 319, loss: 0.3746350407600403, acc: 0.8359375, recall: 0.9230769230769231, precision: 0.7384615384615385, f_beta: 0.8205128205128206\n",
      "train: step: 320, loss: 0.25358641147613525, acc: 0.875, recall: 0.9193548387096774, precision: 0.8382352941176471, f_beta: 0.8769230769230769\n",
      "train: step: 321, loss: 0.25846534967422485, acc: 0.9140625, recall: 0.9846153846153847, precision: 0.8648648648648649, f_beta: 0.920863309352518\n",
      "train: step: 322, loss: 0.30103927850723267, acc: 0.8515625, recall: 0.9166666666666666, precision: 0.7971014492753623, f_beta: 0.8527131782945736\n",
      "train: step: 323, loss: 0.26947250962257385, acc: 0.921875, recall: 0.8983050847457628, precision: 0.9298245614035088, f_beta: 0.9137931034482759\n",
      "train: step: 324, loss: 0.2846395671367645, acc: 0.8671875, recall: 0.8363636363636363, precision: 0.8518518518518519, f_beta: 0.8440366972477065\n",
      "train: step: 325, loss: 0.3335043787956238, acc: 0.859375, recall: 0.8260869565217391, precision: 0.9047619047619048, f_beta: 0.8636363636363636\n",
      "train: step: 326, loss: 0.3662666976451874, acc: 0.8203125, recall: 0.7076923076923077, precision: 0.92, f_beta: 0.8\n",
      "train: step: 327, loss: 0.399829626083374, acc: 0.8203125, recall: 0.6984126984126984, precision: 0.9166666666666666, f_beta: 0.7927927927927927\n",
      "train: step: 328, loss: 0.21996523439884186, acc: 0.921875, recall: 0.8983050847457628, precision: 0.9298245614035088, f_beta: 0.9137931034482759\n",
      "train: step: 329, loss: 0.29214540123939514, acc: 0.875, recall: 0.9, precision: 0.84375, f_beta: 0.870967741935484\n",
      "train: step: 330, loss: 0.2552434206008911, acc: 0.9140625, recall: 0.9428571428571428, precision: 0.9041095890410958, f_beta: 0.923076923076923\n",
      "train: step: 331, loss: 0.30890947580337524, acc: 0.90625, recall: 0.9206349206349206, precision: 0.8923076923076924, f_beta: 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 332, loss: 0.3187878131866455, acc: 0.84375, recall: 0.9454545454545454, precision: 0.7536231884057971, f_beta: 0.8387096774193549\n",
      "train: step: 333, loss: 0.383409321308136, acc: 0.8671875, recall: 0.9672131147540983, precision: 0.7972972972972973, f_beta: 0.874074074074074\n",
      "train: step: 334, loss: 0.28169745206832886, acc: 0.875, recall: 0.8939393939393939, precision: 0.8676470588235294, f_beta: 0.8805970149253731\n",
      "train: step: 335, loss: 0.30600374937057495, acc: 0.875, recall: 0.8805970149253731, precision: 0.8805970149253731, f_beta: 0.8805970149253731\n",
      "train: step: 336, loss: 0.2727561891078949, acc: 0.859375, recall: 0.8367346938775511, precision: 0.803921568627451, f_beta: 0.8200000000000001\n",
      "train: step: 337, loss: 0.2747780680656433, acc: 0.90625, recall: 0.873015873015873, precision: 0.9322033898305084, f_beta: 0.9016393442622951\n",
      "train: step: 338, loss: 0.25368791818618774, acc: 0.8984375, recall: 0.8153846153846154, precision: 0.9814814814814815, f_beta: 0.8907563025210083\n",
      "train: step: 339, loss: 0.3402278423309326, acc: 0.8671875, recall: 0.7538461538461538, precision: 0.98, f_beta: 0.8521739130434782\n",
      "train: step: 340, loss: 0.3057118356227875, acc: 0.875, recall: 0.8333333333333334, precision: 0.9375, f_beta: 0.8823529411764706\n",
      "train: step: 341, loss: 0.32517296075820923, acc: 0.859375, recall: 0.8548387096774194, precision: 0.8548387096774194, f_beta: 0.8548387096774194\n",
      "train: step: 342, loss: 0.306562602519989, acc: 0.875, recall: 0.9016393442622951, precision: 0.8461538461538461, f_beta: 0.873015873015873\n",
      "train: step: 343, loss: 0.3779100179672241, acc: 0.875, recall: 0.9393939393939394, precision: 0.8378378378378378, f_beta: 0.8857142857142858\n",
      "train: step: 344, loss: 0.22456571459770203, acc: 0.90625, recall: 0.9672131147540983, precision: 0.855072463768116, f_beta: 0.9076923076923077\n",
      "train: step: 345, loss: 0.297440767288208, acc: 0.875, recall: 0.9166666666666666, precision: 0.8333333333333334, f_beta: 0.8730158730158729\n",
      "train: step: 346, loss: 0.416523277759552, acc: 0.796875, recall: 0.8727272727272727, precision: 0.7164179104477612, f_beta: 0.7868852459016392\n",
      "train: step: 347, loss: 0.26827481389045715, acc: 0.90625, recall: 0.9137931034482759, precision: 0.8833333333333333, f_beta: 0.8983050847457628\n",
      "train: step: 348, loss: 0.2923525869846344, acc: 0.859375, recall: 0.8194444444444444, precision: 0.921875, f_beta: 0.8676470588235294\n",
      "train: step: 349, loss: 0.3187662363052368, acc: 0.875, recall: 0.821917808219178, precision: 0.9523809523809523, f_beta: 0.8823529411764706\n",
      "train: step: 350, loss: 0.3565191626548767, acc: 0.8515625, recall: 0.7843137254901961, precision: 0.8333333333333334, f_beta: 0.8080808080808081\n",
      "train: step: 351, loss: 0.32095587253570557, acc: 0.859375, recall: 0.8208955223880597, precision: 0.9016393442622951, f_beta: 0.8593750000000001\n",
      "train: step: 352, loss: 0.3296274244785309, acc: 0.828125, recall: 0.7413793103448276, precision: 0.86, f_beta: 0.7962962962962963\n",
      "train: step: 353, loss: 0.37404966354370117, acc: 0.8359375, recall: 0.8169014084507042, precision: 0.8787878787878788, f_beta: 0.8467153284671534\n",
      "train: step: 354, loss: 0.3204226493835449, acc: 0.859375, recall: 0.9310344827586207, precision: 0.7941176470588235, f_beta: 0.8571428571428571\n",
      "train: step: 355, loss: 0.32586869597435, acc: 0.84375, recall: 0.9285714285714286, precision: 0.7647058823529411, f_beta: 0.8387096774193549\n",
      "train: step: 356, loss: 0.3191729485988617, acc: 0.8515625, recall: 0.9285714285714286, precision: 0.7761194029850746, f_beta: 0.8455284552845529\n",
      "train: step: 357, loss: 0.2761516571044922, acc: 0.8828125, recall: 0.9545454545454546, precision: 0.84, f_beta: 0.8936170212765958\n",
      "train: step: 358, loss: 0.28718066215515137, acc: 0.84375, recall: 0.8805970149253731, precision: 0.8309859154929577, f_beta: 0.8550724637681161\n",
      "train: step: 359, loss: 0.2372749149799347, acc: 0.9296875, recall: 0.9166666666666666, precision: 0.9322033898305084, f_beta: 0.9243697478991596\n",
      "train: step: 360, loss: 0.35365790128707886, acc: 0.8671875, recall: 0.8307692307692308, precision: 0.9, f_beta: 0.8640000000000001\n",
      "train: step: 361, loss: 0.2733713984489441, acc: 0.890625, recall: 0.8225806451612904, precision: 0.9444444444444444, f_beta: 0.8793103448275862\n",
      "train: step: 362, loss: 0.250016450881958, acc: 0.9140625, recall: 0.8524590163934426, precision: 0.9629629629629629, f_beta: 0.9043478260869565\n",
      "train: step: 363, loss: 0.2845724821090698, acc: 0.859375, recall: 0.8059701492537313, precision: 0.9152542372881356, f_beta: 0.8571428571428572\n",
      "train: step: 364, loss: 0.34715864062309265, acc: 0.859375, recall: 0.8833333333333333, precision: 0.828125, f_beta: 0.8548387096774193\n",
      "train: step: 365, loss: 0.2781800627708435, acc: 0.890625, recall: 0.8947368421052632, precision: 0.864406779661017, f_beta: 0.8793103448275862\n",
      "train: step: 366, loss: 0.36692389845848083, acc: 0.8359375, recall: 0.8888888888888888, precision: 0.7619047619047619, f_beta: 0.8205128205128205\n",
      "train: step: 367, loss: 0.34763258695602417, acc: 0.8671875, recall: 0.875, precision: 0.8873239436619719, f_beta: 0.881118881118881\n",
      "train: step: 368, loss: 0.219763845205307, acc: 0.9140625, recall: 0.9508196721311475, precision: 0.8787878787878788, f_beta: 0.9133858267716536\n",
      "train: step: 369, loss: 0.2277853935956955, acc: 0.9296875, recall: 0.9206349206349206, precision: 0.9354838709677419, f_beta: 0.9279999999999999\n",
      "train: step: 370, loss: 0.3207550048828125, acc: 0.875, recall: 0.8307692307692308, precision: 0.9152542372881356, f_beta: 0.870967741935484\n",
      "train: step: 371, loss: 0.34271085262298584, acc: 0.8515625, recall: 0.8805970149253731, precision: 0.8428571428571429, f_beta: 0.8613138686131387\n",
      "train: step: 372, loss: 0.2426598221063614, acc: 0.921875, recall: 0.8985507246376812, precision: 0.9538461538461539, f_beta: 0.9253731343283582\n",
      "train: step: 373, loss: 0.3023403286933899, acc: 0.8515625, recall: 0.8382352941176471, precision: 0.8769230769230769, f_beta: 0.8571428571428571\n",
      "train: step: 374, loss: 0.22513744235038757, acc: 0.9375, recall: 0.9545454545454546, precision: 0.9264705882352942, f_beta: 0.9402985074626866\n",
      "train: step: 375, loss: 0.2505751848220825, acc: 0.921875, recall: 0.8870967741935484, precision: 0.9482758620689655, f_beta: 0.9166666666666667\n",
      "train: step: 376, loss: 0.2767125368118286, acc: 0.8828125, recall: 0.9016393442622951, precision: 0.859375, f_beta: 0.88\n",
      "train: step: 377, loss: 0.2732539474964142, acc: 0.8828125, recall: 0.873015873015873, precision: 0.8870967741935484, f_beta: 0.88\n",
      "train: step: 378, loss: 0.28844010829925537, acc: 0.8828125, recall: 0.9047619047619048, precision: 0.8636363636363636, f_beta: 0.8837209302325582\n",
      "train: step: 379, loss: 0.37407755851745605, acc: 0.8203125, recall: 0.8405797101449275, precision: 0.8285714285714286, f_beta: 0.8345323741007195\n",
      "train: step: 380, loss: 0.37777408957481384, acc: 0.8203125, recall: 0.8615384615384616, precision: 0.8, f_beta: 0.8296296296296297\n",
      "train: step: 381, loss: 0.2388482689857483, acc: 0.90625, recall: 0.9016393442622951, precision: 0.9016393442622951, f_beta: 0.9016393442622952\n",
      "train: step: 382, loss: 0.2762258052825928, acc: 0.875, recall: 0.8636363636363636, precision: 0.890625, f_beta: 0.8769230769230768\n",
      "train: step: 383, loss: 0.3012551963329315, acc: 0.8671875, recall: 0.9047619047619048, precision: 0.8382352941176471, f_beta: 0.8702290076335878\n",
      "train: step: 384, loss: 0.2993450164794922, acc: 0.875, recall: 0.855072463768116, precision: 0.9076923076923077, f_beta: 0.8805970149253731\n",
      "train: step: 385, loss: 0.3753196597099304, acc: 0.8359375, recall: 0.8333333333333334, precision: 0.819672131147541, f_beta: 0.8264462809917356\n",
      "train: step: 386, loss: 0.2266152948141098, acc: 0.9140625, recall: 0.9365079365079365, precision: 0.8939393939393939, f_beta: 0.9147286821705426\n",
      "train: step: 387, loss: 0.2614942193031311, acc: 0.890625, recall: 0.8787878787878788, precision: 0.90625, f_beta: 0.8923076923076922\n",
      "train: step: 388, loss: 0.2814810872077942, acc: 0.9296875, recall: 0.8983050847457628, precision: 0.9464285714285714, f_beta: 0.9217391304347826\n",
      "train: step: 389, loss: 0.3227405548095703, acc: 0.8515625, recall: 0.8611111111111112, precision: 0.8732394366197183, f_beta: 0.8671328671328671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 390, loss: 0.30994656682014465, acc: 0.875, recall: 0.9206349206349206, precision: 0.8405797101449275, f_beta: 0.8787878787878787\n",
      "train: step: 391, loss: 0.25098174810409546, acc: 0.9140625, recall: 0.9104477611940298, precision: 0.9242424242424242, f_beta: 0.9172932330827067\n",
      "train: step: 392, loss: 0.2502036392688751, acc: 0.8828125, recall: 0.9122807017543859, precision: 0.8387096774193549, f_beta: 0.8739495798319329\n",
      "train: step: 393, loss: 0.30417096614837646, acc: 0.9296875, recall: 0.9193548387096774, precision: 0.9344262295081968, f_beta: 0.9268292682926829\n",
      "train: step: 394, loss: 0.2656724154949188, acc: 0.875, recall: 0.8870967741935484, precision: 0.859375, f_beta: 0.8730158730158729\n",
      "train: step: 395, loss: 0.2944871783256531, acc: 0.8828125, recall: 0.88, precision: 0.9166666666666666, f_beta: 0.8979591836734694\n",
      "train: step: 396, loss: 0.32764506340026855, acc: 0.8515625, recall: 0.8484848484848485, precision: 0.8615384615384616, f_beta: 0.8549618320610687\n",
      "train: step: 397, loss: 0.23640033602714539, acc: 0.921875, recall: 0.9310344827586207, precision: 0.9, f_beta: 0.9152542372881356\n",
      "train: step: 398, loss: 0.22010710835456848, acc: 0.8984375, recall: 0.8070175438596491, precision: 0.9583333333333334, f_beta: 0.8761904761904762\n",
      "train: step: 399, loss: 0.24623070657253265, acc: 0.921875, recall: 0.9324324324324325, precision: 0.9324324324324325, f_beta: 0.9324324324324325\n",
      "train: step: 400, loss: 0.23682139813899994, acc: 0.921875, recall: 0.9078947368421053, precision: 0.9583333333333334, f_beta: 0.9324324324324325\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:35:22.581916, step: 400, loss: 0.33503887133720595, acc: 0.8519631410256411,precision: 0.8942658359227642, recall: 0.8275851386716266, f_beta: 0.8584468512437925\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-400\n",
      "\n",
      "train: step: 401, loss: 0.23184901475906372, acc: 0.90625, recall: 0.9428571428571428, precision: 0.8918918918918919, f_beta: 0.9166666666666667\n",
      "train: step: 402, loss: 0.2935742735862732, acc: 0.859375, recall: 0.8769230769230769, precision: 0.8507462686567164, f_beta: 0.8636363636363636\n",
      "train: step: 403, loss: 0.3850799798965454, acc: 0.828125, recall: 0.8947368421052632, precision: 0.7611940298507462, f_beta: 0.8225806451612904\n",
      "train: step: 404, loss: 0.47266775369644165, acc: 0.8125, recall: 0.8823529411764706, precision: 0.7894736842105263, f_beta: 0.8333333333333333\n",
      "train: step: 405, loss: 0.37251710891723633, acc: 0.8359375, recall: 0.8923076923076924, precision: 0.8055555555555556, f_beta: 0.8467153284671534\n",
      "train: step: 406, loss: 0.28465723991394043, acc: 0.8828125, recall: 0.9076923076923077, precision: 0.8676470588235294, f_beta: 0.887218045112782\n",
      "train: step: 407, loss: 0.24749918282032013, acc: 0.875, recall: 0.8153846153846154, precision: 0.9298245614035088, f_beta: 0.8688524590163934\n",
      "train: step: 408, loss: 0.28690239787101746, acc: 0.8359375, recall: 0.7454545454545455, precision: 0.8541666666666666, f_beta: 0.7961165048543689\n",
      "train: step: 409, loss: 0.27306699752807617, acc: 0.8671875, recall: 0.7966101694915254, precision: 0.9038461538461539, f_beta: 0.8468468468468469\n",
      "train: step: 410, loss: 0.3120848834514618, acc: 0.859375, recall: 0.8, precision: 0.9122807017543859, f_beta: 0.8524590163934427\n",
      "train: step: 411, loss: 0.24083122611045837, acc: 0.9140625, recall: 0.875, precision: 0.9491525423728814, f_beta: 0.9105691056910569\n",
      "train: step: 412, loss: 0.256122887134552, acc: 0.8828125, recall: 0.8813559322033898, precision: 0.8666666666666667, f_beta: 0.8739495798319329\n",
      "train: step: 413, loss: 0.33577701449394226, acc: 0.859375, recall: 0.8823529411764706, precision: 0.8571428571428571, f_beta: 0.8695652173913043\n",
      "train: step: 414, loss: 0.40994328260421753, acc: 0.828125, recall: 0.8939393939393939, precision: 0.7972972972972973, f_beta: 0.8428571428571429\n",
      "train: step: 415, loss: 0.22897869348526, acc: 0.921875, recall: 0.9545454545454546, precision: 0.9, f_beta: 0.9264705882352942\n",
      "train: step: 416, loss: 0.3394015431404114, acc: 0.84375, recall: 0.9459459459459459, precision: 0.813953488372093, f_beta: 0.875\n",
      "train: step: 417, loss: 0.29927611351013184, acc: 0.8671875, recall: 0.8852459016393442, precision: 0.84375, f_beta: 0.864\n",
      "train: step: 418, loss: 0.2356627881526947, acc: 0.90625, recall: 0.9242424242424242, precision: 0.8970588235294118, f_beta: 0.9104477611940298\n",
      "train: step: 419, loss: 0.2883622646331787, acc: 0.8515625, recall: 0.821917808219178, precision: 0.9090909090909091, f_beta: 0.8633093525179857\n",
      "train: step: 420, loss: 0.21525417268276215, acc: 0.9296875, recall: 0.8947368421052632, precision: 0.9444444444444444, f_beta: 0.918918918918919\n",
      "train: step: 421, loss: 0.24243877828121185, acc: 0.9140625, recall: 0.9264705882352942, precision: 0.9130434782608695, f_beta: 0.9197080291970804\n",
      "train: step: 422, loss: 0.3167940378189087, acc: 0.859375, recall: 0.8714285714285714, precision: 0.8714285714285714, f_beta: 0.8714285714285714\n",
      "train: step: 423, loss: 0.2682950794696808, acc: 0.875, recall: 0.8732394366197183, precision: 0.8985507246376812, f_beta: 0.8857142857142857\n",
      "train: step: 424, loss: 0.3072832226753235, acc: 0.8515625, recall: 0.8888888888888888, precision: 0.7868852459016393, f_beta: 0.8347826086956522\n",
      "train: step: 425, loss: 0.31632643938064575, acc: 0.875, recall: 0.9384615384615385, precision: 0.8356164383561644, f_beta: 0.8840579710144928\n",
      "train: step: 426, loss: 0.22812387347221375, acc: 0.921875, recall: 0.9444444444444444, precision: 0.918918918918919, f_beta: 0.9315068493150684\n",
      "train: step: 427, loss: 0.24607747793197632, acc: 0.90625, recall: 0.8775510204081632, precision: 0.8775510204081632, f_beta: 0.8775510204081631\n",
      "train: step: 428, loss: 0.24565160274505615, acc: 0.8984375, recall: 0.8769230769230769, precision: 0.9193548387096774, f_beta: 0.8976377952755904\n",
      "train: step: 429, loss: 0.257610023021698, acc: 0.90625, recall: 0.8939393939393939, precision: 0.921875, f_beta: 0.9076923076923077\n",
      "train: step: 430, loss: 0.31268149614334106, acc: 0.8984375, recall: 0.8813559322033898, precision: 0.896551724137931, f_beta: 0.888888888888889\n",
      "train: step: 431, loss: 0.2562114894390106, acc: 0.9140625, recall: 0.9104477611940298, precision: 0.9242424242424242, f_beta: 0.9172932330827067\n",
      "train: step: 432, loss: 0.2888321280479431, acc: 0.8515625, recall: 0.8260869565217391, precision: 0.890625, f_beta: 0.8571428571428571\n",
      "train: step: 433, loss: 0.2133219689130783, acc: 0.8984375, recall: 0.9310344827586207, precision: 0.8571428571428571, f_beta: 0.8925619834710743\n",
      "train: step: 434, loss: 0.2057414948940277, acc: 0.9375, recall: 0.9344262295081968, precision: 0.9344262295081968, f_beta: 0.9344262295081968\n",
      "train: step: 435, loss: 0.31984996795654297, acc: 0.8671875, recall: 0.8524590163934426, precision: 0.8666666666666667, f_beta: 0.8595041322314049\n",
      "train: step: 436, loss: 0.2778303027153015, acc: 0.875, recall: 0.9310344827586207, precision: 0.8181818181818182, f_beta: 0.8709677419354839\n",
      "train: step: 437, loss: 0.2172505259513855, acc: 0.9453125, recall: 0.927536231884058, precision: 0.9696969696969697, f_beta: 0.9481481481481481\n",
      "train: step: 438, loss: 0.290081262588501, acc: 0.90625, recall: 0.90625, precision: 0.90625, f_beta: 0.90625\n",
      "train: step: 439, loss: 0.3645261228084564, acc: 0.8203125, recall: 0.8518518518518519, precision: 0.7540983606557377, f_beta: 0.7999999999999999\n",
      "train: step: 440, loss: 0.27738267183303833, acc: 0.90625, recall: 0.9393939393939394, precision: 0.8857142857142857, f_beta: 0.9117647058823529\n",
      "train: step: 441, loss: 0.2305598109960556, acc: 0.9140625, recall: 0.9516129032258065, precision: 0.8805970149253731, f_beta: 0.9147286821705426\n",
      "train: step: 442, loss: 0.3516726493835449, acc: 0.8515625, recall: 0.8245614035087719, precision: 0.8392857142857143, f_beta: 0.8318584070796461\n",
      "train: step: 443, loss: 0.2764168083667755, acc: 0.84375, recall: 0.7714285714285715, precision: 0.9310344827586207, f_beta: 0.8437500000000001\n",
      "train: step: 444, loss: 0.32022905349731445, acc: 0.8984375, recall: 0.8783783783783784, precision: 0.9420289855072463, f_beta: 0.9090909090909092\n",
      "train: step: 445, loss: 0.1955561637878418, acc: 0.9140625, recall: 0.9393939393939394, precision: 0.8985507246376812, f_beta: 0.9185185185185185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 446, loss: 0.21077023446559906, acc: 0.9296875, recall: 0.9838709677419355, precision: 0.8840579710144928, f_beta: 0.9312977099236642\n",
      "train: step: 447, loss: 0.3814088702201843, acc: 0.84375, recall: 0.9047619047619048, precision: 0.8028169014084507, f_beta: 0.8507462686567164\n",
      "train: step: 448, loss: 0.3231103718280792, acc: 0.8671875, recall: 0.9047619047619048, precision: 0.8382352941176471, f_beta: 0.8702290076335878\n",
      "train: step: 449, loss: 0.24292263388633728, acc: 0.890625, recall: 0.967741935483871, precision: 0.8333333333333334, f_beta: 0.8955223880597015\n",
      "train: step: 450, loss: 0.3444453179836273, acc: 0.859375, recall: 0.8235294117647058, precision: 0.9032258064516129, f_beta: 0.8615384615384616\n",
      "train: step: 451, loss: 0.21556100249290466, acc: 0.9453125, recall: 0.9166666666666666, precision: 0.9649122807017544, f_beta: 0.9401709401709402\n",
      "train: step: 452, loss: 0.3229309320449829, acc: 0.8671875, recall: 0.864406779661017, precision: 0.85, f_beta: 0.8571428571428572\n",
      "train: step: 453, loss: 0.2513918876647949, acc: 0.890625, recall: 0.8985507246376812, precision: 0.8985507246376812, f_beta: 0.8985507246376812\n",
      "train: step: 454, loss: 0.34343820810317993, acc: 0.84375, recall: 0.8805970149253731, precision: 0.8309859154929577, f_beta: 0.8550724637681161\n",
      "train: step: 455, loss: 0.2821809649467468, acc: 0.859375, recall: 0.84375, precision: 0.8709677419354839, f_beta: 0.8571428571428571\n",
      "train: step: 456, loss: 0.34878113865852356, acc: 0.8359375, recall: 0.8392857142857143, precision: 0.7966101694915254, f_beta: 0.8173913043478261\n",
      "train: step: 457, loss: 0.3262050449848175, acc: 0.84375, recall: 0.88, precision: 0.8571428571428571, f_beta: 0.8684210526315789\n",
      "train: step: 458, loss: 0.27237653732299805, acc: 0.8828125, recall: 0.8769230769230769, precision: 0.890625, f_beta: 0.883720930232558\n",
      "train: step: 459, loss: 0.24383126199245453, acc: 0.9296875, recall: 0.9264705882352942, precision: 0.9402985074626866, f_beta: 0.9333333333333335\n",
      "train: step: 460, loss: 0.19120976328849792, acc: 0.9453125, recall: 0.9696969696969697, precision: 0.927536231884058, f_beta: 0.9481481481481481\n",
      "train: step: 461, loss: 0.2822747826576233, acc: 0.8671875, recall: 0.8695652173913043, precision: 0.8823529411764706, f_beta: 0.8759124087591241\n",
      "train: step: 462, loss: 0.235279381275177, acc: 0.90625, recall: 0.9122807017543859, precision: 0.8813559322033898, f_beta: 0.8965517241379309\n",
      "train: step: 463, loss: 0.22221392393112183, acc: 0.9140625, recall: 0.9206349206349206, precision: 0.90625, f_beta: 0.9133858267716536\n",
      "train: step: 464, loss: 0.22374853491783142, acc: 0.9140625, recall: 0.9333333333333333, precision: 0.9210526315789473, f_beta: 0.9271523178807947\n",
      "train: step: 465, loss: 0.2197594791650772, acc: 0.9296875, recall: 0.9411764705882353, precision: 0.927536231884058, f_beta: 0.9343065693430658\n",
      "train: step: 466, loss: 0.21421843767166138, acc: 0.90625, recall: 0.921875, precision: 0.8939393939393939, f_beta: 0.9076923076923077\n",
      "train: step: 467, loss: 0.2861725091934204, acc: 0.8828125, recall: 0.9375, precision: 0.8823529411764706, f_beta: 0.9090909090909091\n",
      "train: step: 468, loss: 0.28075456619262695, acc: 0.875, recall: 0.8955223880597015, precision: 0.8695652173913043, f_beta: 0.8823529411764706\n",
      "start training model\n",
      "train: step: 469, loss: 0.24188923835754395, acc: 0.9140625, recall: 0.9538461538461539, precision: 0.8857142857142857, f_beta: 0.9185185185185185\n",
      "train: step: 470, loss: 0.19792379438877106, acc: 0.90625, recall: 0.875, precision: 0.9545454545454546, f_beta: 0.9130434782608695\n",
      "train: step: 471, loss: 0.2872350215911865, acc: 0.921875, recall: 0.9041095890410958, precision: 0.9565217391304348, f_beta: 0.9295774647887325\n",
      "train: step: 472, loss: 0.17082998156547546, acc: 0.953125, recall: 0.9696969696969697, precision: 0.9411764705882353, f_beta: 0.955223880597015\n",
      "train: step: 473, loss: 0.2239208221435547, acc: 0.9140625, recall: 0.9516129032258065, precision: 0.8805970149253731, f_beta: 0.9147286821705426\n",
      "train: step: 474, loss: 0.23158392310142517, acc: 0.90625, recall: 0.9552238805970149, precision: 0.8767123287671232, f_beta: 0.9142857142857143\n",
      "train: step: 475, loss: 0.18109150230884552, acc: 0.953125, recall: 0.9722222222222222, precision: 0.9459459459459459, f_beta: 0.9589041095890412\n",
      "train: step: 476, loss: 0.18765567243099213, acc: 0.921875, recall: 0.9508196721311475, precision: 0.8923076923076924, f_beta: 0.9206349206349206\n",
      "train: step: 477, loss: 0.17159275710582733, acc: 0.9375, recall: 0.9705882352941176, precision: 0.9166666666666666, f_beta: 0.9428571428571428\n",
      "train: step: 478, loss: 0.22441810369491577, acc: 0.921875, recall: 0.90625, precision: 0.9354838709677419, f_beta: 0.9206349206349206\n",
      "train: step: 479, loss: 0.2799980044364929, acc: 0.8984375, recall: 0.8888888888888888, precision: 0.8727272727272727, f_beta: 0.8807339449541284\n",
      "train: step: 480, loss: 0.21852801740169525, acc: 0.9140625, recall: 0.8955223880597015, precision: 0.9375, f_beta: 0.9160305343511451\n",
      "train: step: 481, loss: 0.22790031135082245, acc: 0.890625, recall: 0.8676470588235294, precision: 0.921875, f_beta: 0.893939393939394\n",
      "train: step: 482, loss: 0.23265668749809265, acc: 0.921875, recall: 0.8870967741935484, precision: 0.9482758620689655, f_beta: 0.9166666666666667\n",
      "train: step: 483, loss: 0.2523559629917145, acc: 0.9140625, recall: 0.9193548387096774, precision: 0.9047619047619048, f_beta: 0.912\n",
      "train: step: 484, loss: 0.1566448211669922, acc: 0.953125, recall: 0.9384615384615385, precision: 0.9682539682539683, f_beta: 0.953125\n",
      "train: step: 485, loss: 0.23130935430526733, acc: 0.9296875, recall: 0.9508196721311475, precision: 0.90625, f_beta: 0.9279999999999999\n",
      "train: step: 486, loss: 0.2502502202987671, acc: 0.8984375, recall: 0.9333333333333333, precision: 0.8615384615384616, f_beta: 0.8960000000000001\n",
      "train: step: 487, loss: 0.21119819581508636, acc: 0.9140625, recall: 0.9523809523809523, precision: 0.8823529411764706, f_beta: 0.916030534351145\n",
      "train: step: 488, loss: 0.22380948066711426, acc: 0.90625, recall: 0.9420289855072463, precision: 0.8904109589041096, f_beta: 0.9154929577464788\n",
      "train: step: 489, loss: 0.2525072991847992, acc: 0.8984375, recall: 0.9420289855072463, precision: 0.8783783783783784, f_beta: 0.9090909090909092\n",
      "train: step: 490, loss: 0.19488918781280518, acc: 0.9296875, recall: 0.8703703703703703, precision: 0.9591836734693877, f_beta: 0.912621359223301\n",
      "train: step: 491, loss: 0.2850983440876007, acc: 0.875, recall: 0.8552631578947368, precision: 0.9285714285714286, f_beta: 0.8904109589041096\n",
      "train: step: 492, loss: 0.18882620334625244, acc: 0.90625, recall: 0.9076923076923077, precision: 0.9076923076923077, f_beta: 0.9076923076923076\n",
      "train: step: 493, loss: 0.14124397933483124, acc: 0.9453125, recall: 0.9295774647887324, precision: 0.9705882352941176, f_beta: 0.9496402877697842\n",
      "train: step: 494, loss: 0.19715836644172668, acc: 0.9140625, recall: 0.8947368421052632, precision: 0.9107142857142857, f_beta: 0.9026548672566371\n",
      "train: step: 495, loss: 0.25643983483314514, acc: 0.8984375, recall: 0.9285714285714286, precision: 0.8524590163934426, f_beta: 0.888888888888889\n",
      "train: step: 496, loss: 0.2332862764596939, acc: 0.9140625, recall: 0.9411764705882353, precision: 0.9014084507042254, f_beta: 0.920863309352518\n",
      "train: step: 497, loss: 0.2675336003303528, acc: 0.875, recall: 0.864406779661017, precision: 0.864406779661017, f_beta: 0.864406779661017\n",
      "train: step: 498, loss: 0.20549245178699493, acc: 0.9140625, recall: 0.8955223880597015, precision: 0.9375, f_beta: 0.9160305343511451\n",
      "train: step: 499, loss: 0.2546842694282532, acc: 0.8984375, recall: 0.8983050847457628, precision: 0.8833333333333333, f_beta: 0.8907563025210085\n",
      "train: step: 500, loss: 0.1582634150981903, acc: 0.9609375, recall: 0.9705882352941176, precision: 0.9565217391304348, f_beta: 0.9635036496350365\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:35:41.905333, step: 500, loss: 0.32539906257238144, acc: 0.8635817307692307,precision: 0.8462488604733297, recall: 0.879033739213189, f_beta: 0.8616965267883353\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-500\n",
      "\n",
      "train: step: 501, loss: 0.25776252150535583, acc: 0.921875, recall: 0.9193548387096774, precision: 0.9193548387096774, f_beta: 0.9193548387096774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 502, loss: 0.16527652740478516, acc: 0.96875, recall: 0.9516129032258065, precision: 0.9833333333333333, f_beta: 0.9672131147540983\n",
      "train: step: 503, loss: 0.21123389899730682, acc: 0.921875, recall: 0.890625, precision: 0.95, f_beta: 0.9193548387096774\n",
      "train: step: 504, loss: 0.20171761512756348, acc: 0.890625, recall: 0.90625, precision: 0.8787878787878788, f_beta: 0.8923076923076922\n",
      "train: step: 505, loss: 0.19482241570949554, acc: 0.953125, recall: 0.9846153846153847, precision: 0.927536231884058, f_beta: 0.9552238805970149\n",
      "train: step: 506, loss: 0.23048226535320282, acc: 0.890625, recall: 0.9298245614035088, precision: 0.8412698412698413, f_beta: 0.8833333333333334\n",
      "train: step: 507, loss: 0.22545897960662842, acc: 0.9375, recall: 0.9682539682539683, precision: 0.9104477611940298, f_beta: 0.9384615384615386\n",
      "train: step: 508, loss: 0.2485375702381134, acc: 0.9140625, recall: 0.921875, precision: 0.9076923076923077, f_beta: 0.9147286821705427\n",
      "train: step: 509, loss: 0.16974784433841705, acc: 0.9609375, recall: 0.9701492537313433, precision: 0.9558823529411765, f_beta: 0.962962962962963\n",
      "train: step: 510, loss: 0.13592606782913208, acc: 0.953125, recall: 0.9428571428571428, precision: 0.9705882352941176, f_beta: 0.9565217391304348\n",
      "train: step: 511, loss: 0.26312339305877686, acc: 0.875, recall: 0.8305084745762712, precision: 0.8909090909090909, f_beta: 0.8596491228070176\n",
      "train: step: 512, loss: 0.23045390844345093, acc: 0.9375, recall: 0.9076923076923077, precision: 0.9672131147540983, f_beta: 0.9365079365079365\n",
      "train: step: 513, loss: 0.2146272361278534, acc: 0.921875, recall: 0.9538461538461539, precision: 0.8985507246376812, f_beta: 0.9253731343283582\n",
      "train: step: 514, loss: 0.2294701337814331, acc: 0.9140625, recall: 0.8793103448275862, precision: 0.9272727272727272, f_beta: 0.902654867256637\n",
      "train: step: 515, loss: 0.20511984825134277, acc: 0.90625, recall: 0.9365079365079365, precision: 0.8805970149253731, f_beta: 0.9076923076923077\n",
      "train: step: 516, loss: 0.2083546370267868, acc: 0.9375, recall: 0.9436619718309859, precision: 0.9436619718309859, f_beta: 0.9436619718309859\n",
      "train: step: 517, loss: 0.16313600540161133, acc: 0.9609375, recall: 0.967741935483871, precision: 0.9523809523809523, f_beta: 0.96\n",
      "train: step: 518, loss: 0.17362052202224731, acc: 0.9453125, recall: 0.9672131147540983, precision: 0.921875, f_beta: 0.944\n",
      "train: step: 519, loss: 0.2535104751586914, acc: 0.90625, recall: 0.9253731343283582, precision: 0.8985507246376812, f_beta: 0.9117647058823529\n",
      "train: step: 520, loss: 0.28829503059387207, acc: 0.890625, recall: 0.875, precision: 0.9264705882352942, f_beta: 0.9\n",
      "train: step: 521, loss: 0.17386363446712494, acc: 0.9296875, recall: 0.9076923076923077, precision: 0.9516129032258065, f_beta: 0.9291338582677167\n",
      "train: step: 522, loss: 0.24066919088363647, acc: 0.90625, recall: 0.9253731343283582, precision: 0.8985507246376812, f_beta: 0.9117647058823529\n",
      "train: step: 523, loss: 0.25169551372528076, acc: 0.8984375, recall: 0.8787878787878788, precision: 0.9206349206349206, f_beta: 0.8992248062015504\n",
      "train: step: 524, loss: 0.179362490773201, acc: 0.9453125, recall: 0.9107142857142857, precision: 0.9622641509433962, f_beta: 0.9357798165137615\n",
      "train: step: 525, loss: 0.18481488525867462, acc: 0.9609375, recall: 0.9655172413793104, precision: 0.9491525423728814, f_beta: 0.9572649572649573\n",
      "train: step: 526, loss: 0.22951999306678772, acc: 0.921875, recall: 0.9253731343283582, precision: 0.9253731343283582, f_beta: 0.9253731343283582\n",
      "train: step: 527, loss: 0.20960134267807007, acc: 0.9375, recall: 0.9411764705882353, precision: 0.9411764705882353, f_beta: 0.9411764705882353\n",
      "train: step: 528, loss: 0.20911985635757446, acc: 0.9375, recall: 0.9682539682539683, precision: 0.9104477611940298, f_beta: 0.9384615384615386\n",
      "train: step: 529, loss: 0.1663842499256134, acc: 0.9375, recall: 0.9242424242424242, precision: 0.953125, f_beta: 0.9384615384615383\n",
      "train: step: 530, loss: 0.23701220750808716, acc: 0.90625, recall: 0.9032258064516129, precision: 0.9032258064516129, f_beta: 0.9032258064516129\n",
      "train: step: 531, loss: 0.16652986407279968, acc: 0.953125, recall: 0.972972972972973, precision: 0.9473684210526315, f_beta: 0.9599999999999999\n",
      "train: step: 532, loss: 0.1842505931854248, acc: 0.9296875, recall: 0.9420289855072463, precision: 0.9285714285714286, f_beta: 0.935251798561151\n",
      "train: step: 533, loss: 0.21167618036270142, acc: 0.921875, recall: 0.9322033898305084, precision: 0.9016393442622951, f_beta: 0.9166666666666666\n",
      "train: step: 534, loss: 0.19844716787338257, acc: 0.90625, recall: 0.9154929577464789, precision: 0.9154929577464789, f_beta: 0.9154929577464789\n",
      "train: step: 535, loss: 0.2440095841884613, acc: 0.9140625, recall: 0.9047619047619048, precision: 0.9193548387096774, f_beta: 0.912\n",
      "train: step: 536, loss: 0.23499834537506104, acc: 0.9140625, recall: 0.9464285714285714, precision: 0.8688524590163934, f_beta: 0.9059829059829059\n",
      "train: step: 537, loss: 0.20363177359104156, acc: 0.9140625, recall: 0.9344262295081968, precision: 0.890625, f_beta: 0.9120000000000001\n",
      "train: step: 538, loss: 0.28135040402412415, acc: 0.90625, recall: 0.9090909090909091, precision: 0.9090909090909091, f_beta: 0.9090909090909091\n",
      "train: step: 539, loss: 0.1417095810174942, acc: 0.9609375, recall: 0.96875, precision: 0.9538461538461539, f_beta: 0.9612403100775193\n",
      "train: step: 540, loss: 0.21665740013122559, acc: 0.9296875, recall: 0.9310344827586207, precision: 0.9152542372881356, f_beta: 0.923076923076923\n",
      "train: step: 541, loss: 0.25166967511177063, acc: 0.90625, recall: 0.84375, precision: 0.9642857142857143, f_beta: 0.8999999999999999\n",
      "train: step: 542, loss: 0.20955592393875122, acc: 0.921875, recall: 0.9152542372881356, precision: 0.9152542372881356, f_beta: 0.9152542372881356\n",
      "train: step: 543, loss: 0.2506580352783203, acc: 0.90625, recall: 0.8947368421052632, precision: 0.8947368421052632, f_beta: 0.8947368421052632\n",
      "train: step: 544, loss: 0.1936967819929123, acc: 0.9296875, recall: 0.9452054794520548, precision: 0.9324324324324325, f_beta: 0.9387755102040816\n",
      "train: step: 545, loss: 0.1412200778722763, acc: 0.9765625, recall: 1.0, precision: 0.9594594594594594, f_beta: 0.9793103448275862\n",
      "train: step: 546, loss: 0.25898730754852295, acc: 0.90625, recall: 0.9655172413793104, precision: 0.8484848484848485, f_beta: 0.9032258064516129\n",
      "train: step: 547, loss: 0.171145498752594, acc: 0.9296875, recall: 0.9846153846153847, precision: 0.8888888888888888, f_beta: 0.9343065693430657\n",
      "train: step: 548, loss: 0.1929905265569687, acc: 0.9296875, recall: 0.927536231884058, precision: 0.9411764705882353, f_beta: 0.9343065693430658\n",
      "train: step: 549, loss: 0.21500669419765472, acc: 0.9296875, recall: 0.9523809523809523, precision: 0.9090909090909091, f_beta: 0.9302325581395349\n",
      "train: step: 550, loss: 0.25753045082092285, acc: 0.921875, recall: 0.927536231884058, precision: 0.927536231884058, f_beta: 0.927536231884058\n",
      "train: step: 551, loss: 0.20383498072624207, acc: 0.9375, recall: 0.9180327868852459, precision: 0.9491525423728814, f_beta: 0.9333333333333333\n",
      "train: step: 552, loss: 0.18761791288852692, acc: 0.9296875, recall: 0.9117647058823529, precision: 0.9538461538461539, f_beta: 0.9323308270676691\n",
      "train: step: 553, loss: 0.23492293059825897, acc: 0.8671875, recall: 0.8852459016393442, precision: 0.84375, f_beta: 0.864\n",
      "train: step: 554, loss: 0.24990424513816833, acc: 0.90625, recall: 0.8939393939393939, precision: 0.921875, f_beta: 0.9076923076923077\n",
      "train: step: 555, loss: 0.25927597284317017, acc: 0.890625, recall: 0.9019607843137255, precision: 0.8363636363636363, f_beta: 0.8679245283018867\n",
      "train: step: 556, loss: 0.22581668198108673, acc: 0.9375, recall: 0.9733333333333334, precision: 0.9240506329113924, f_beta: 0.948051948051948\n",
      "train: step: 557, loss: 0.18781691789627075, acc: 0.9453125, recall: 0.9130434782608695, precision: 0.984375, f_beta: 0.9473684210526315\n",
      "train: step: 558, loss: 0.16549067199230194, acc: 0.9453125, recall: 0.9726027397260274, precision: 0.9342105263157895, f_beta: 0.9530201342281879\n",
      "train: step: 559, loss: 0.23293422162532806, acc: 0.8984375, recall: 0.9285714285714286, precision: 0.8904109589041096, f_beta: 0.9090909090909091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 560, loss: 0.20089489221572876, acc: 0.9296875, recall: 0.9830508474576272, precision: 0.8787878787878788, f_beta: 0.9279999999999999\n",
      "train: step: 561, loss: 0.25608041882514954, acc: 0.90625, recall: 0.9152542372881356, precision: 0.8852459016393442, f_beta: 0.9\n",
      "train: step: 562, loss: 0.2453487515449524, acc: 0.90625, recall: 0.8923076923076924, precision: 0.9206349206349206, f_beta: 0.90625\n",
      "train: step: 563, loss: 0.21458278596401215, acc: 0.9140625, recall: 0.8703703703703703, precision: 0.9215686274509803, f_beta: 0.8952380952380952\n",
      "train: step: 564, loss: 0.2325664460659027, acc: 0.890625, recall: 0.8421052631578947, precision: 0.9056603773584906, f_beta: 0.8727272727272727\n",
      "train: step: 565, loss: 0.15681901574134827, acc: 0.9453125, recall: 0.9354838709677419, precision: 0.9508196721311475, f_beta: 0.943089430894309\n",
      "train: step: 566, loss: 0.2243158221244812, acc: 0.9140625, recall: 0.9482758620689655, precision: 0.873015873015873, f_beta: 0.9090909090909091\n",
      "train: step: 567, loss: 0.25098609924316406, acc: 0.9375, recall: 0.9014084507042254, precision: 0.9846153846153847, f_beta: 0.9411764705882353\n",
      "train: step: 568, loss: 0.19820934534072876, acc: 0.921875, recall: 0.967741935483871, precision: 0.8823529411764706, f_beta: 0.923076923076923\n",
      "train: step: 569, loss: 0.2119370549917221, acc: 0.9140625, recall: 0.9473684210526315, precision: 0.8709677419354839, f_beta: 0.9075630252100839\n",
      "train: step: 570, loss: 0.15996983647346497, acc: 0.953125, recall: 0.9344262295081968, precision: 0.9661016949152542, f_beta: 0.95\n",
      "train: step: 571, loss: 0.20346567034721375, acc: 0.9140625, recall: 0.9365079365079365, precision: 0.8939393939393939, f_beta: 0.9147286821705426\n",
      "train: step: 572, loss: 0.1651473045349121, acc: 0.9375, recall: 0.927536231884058, precision: 0.9552238805970149, f_beta: 0.9411764705882353\n",
      "train: step: 573, loss: 0.16944129765033722, acc: 0.953125, recall: 0.9508196721311475, precision: 0.9508196721311475, f_beta: 0.9508196721311475\n",
      "train: step: 574, loss: 0.2994810938835144, acc: 0.890625, recall: 0.8888888888888888, precision: 0.9142857142857143, f_beta: 0.9014084507042254\n",
      "train: step: 575, loss: 0.24120740592479706, acc: 0.9140625, recall: 0.9420289855072463, precision: 0.9027777777777778, f_beta: 0.9219858156028369\n",
      "train: step: 576, loss: 0.18390433490276337, acc: 0.921875, recall: 0.9107142857142857, precision: 0.9107142857142857, f_beta: 0.9107142857142857\n",
      "train: step: 577, loss: 0.17960405349731445, acc: 0.953125, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001\n",
      "train: step: 578, loss: 0.16786891222000122, acc: 0.9375, recall: 0.9607843137254902, precision: 0.8909090909090909, f_beta: 0.9245283018867925\n",
      "train: step: 579, loss: 0.18693868815898895, acc: 0.90625, recall: 0.9836065573770492, precision: 0.8450704225352113, f_beta: 0.9090909090909091\n",
      "train: step: 580, loss: 0.22297102212905884, acc: 0.90625, recall: 0.9142857142857143, precision: 0.9142857142857143, f_beta: 0.9142857142857143\n",
      "train: step: 581, loss: 0.1706341803073883, acc: 0.921875, recall: 0.8813559322033898, precision: 0.9454545454545454, f_beta: 0.9122807017543859\n",
      "train: step: 582, loss: 0.18020139634609222, acc: 0.9296875, recall: 0.9846153846153847, precision: 0.8888888888888888, f_beta: 0.9343065693430657\n",
      "train: step: 583, loss: 0.24859404563903809, acc: 0.890625, recall: 0.8571428571428571, precision: 0.9565217391304348, f_beta: 0.904109589041096\n",
      "train: step: 584, loss: 0.2038736641407013, acc: 0.9375, recall: 0.9538461538461539, precision: 0.9253731343283582, f_beta: 0.9393939393939394\n",
      "train: step: 585, loss: 0.24187079071998596, acc: 0.8984375, recall: 0.9411764705882353, precision: 0.8275862068965517, f_beta: 0.8807339449541284\n",
      "train: step: 586, loss: 0.19583186507225037, acc: 0.9140625, recall: 0.9464285714285714, precision: 0.8688524590163934, f_beta: 0.9059829059829059\n",
      "train: step: 587, loss: 0.24951171875, acc: 0.890625, recall: 0.8360655737704918, precision: 0.9272727272727272, f_beta: 0.8793103448275862\n",
      "train: step: 588, loss: 0.16636469960212708, acc: 0.9375, recall: 0.9206349206349206, precision: 0.9508196721311475, f_beta: 0.9354838709677418\n",
      "train: step: 589, loss: 0.14517441391944885, acc: 0.953125, recall: 0.9696969696969697, precision: 0.9411764705882353, f_beta: 0.955223880597015\n",
      "train: step: 590, loss: 0.26785778999328613, acc: 0.875, recall: 0.8888888888888888, precision: 0.8888888888888888, f_beta: 0.8888888888888888\n",
      "train: step: 591, loss: 0.18088792264461517, acc: 0.921875, recall: 0.9090909090909091, precision: 0.9375, f_beta: 0.923076923076923\n",
      "train: step: 592, loss: 0.24306172132492065, acc: 0.8828125, recall: 0.8888888888888888, precision: 0.875, f_beta: 0.8818897637795274\n",
      "train: step: 593, loss: 0.16146063804626465, acc: 0.9453125, recall: 0.95, precision: 0.9344262295081968, f_beta: 0.9421487603305784\n",
      "train: step: 594, loss: 0.19067077338695526, acc: 0.921875, recall: 0.953125, precision: 0.8970588235294118, f_beta: 0.9242424242424244\n",
      "train: step: 595, loss: 0.22918210923671722, acc: 0.8984375, recall: 0.9636363636363636, precision: 0.828125, f_beta: 0.8907563025210083\n",
      "train: step: 596, loss: 0.25657638907432556, acc: 0.8671875, recall: 0.8181818181818182, precision: 0.8653846153846154, f_beta: 0.8411214953271028\n",
      "train: step: 597, loss: 0.18382471799850464, acc: 0.921875, recall: 0.90625, precision: 0.9354838709677419, f_beta: 0.9206349206349206\n",
      "train: step: 598, loss: 0.15134352445602417, acc: 0.9453125, recall: 0.9253731343283582, precision: 0.96875, f_beta: 0.9465648854961832\n",
      "train: step: 599, loss: 0.24825508892536163, acc: 0.890625, recall: 0.8225806451612904, precision: 0.9444444444444444, f_beta: 0.8793103448275862\n",
      "train: step: 600, loss: 0.1660456359386444, acc: 0.953125, recall: 0.9411764705882353, precision: 0.9696969696969697, f_beta: 0.955223880597015\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:36:01.397572, step: 600, loss: 0.3146323492893806, acc: 0.8629807692307693,precision: 0.8709089056921707, recall: 0.8608550162023397, f_beta: 0.8647082344656343\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-600\n",
      "\n",
      "train: step: 601, loss: 0.2735799252986908, acc: 0.9140625, recall: 0.875, precision: 0.9692307692307692, f_beta: 0.9197080291970802\n",
      "train: step: 602, loss: 0.17226214706897736, acc: 0.9375, recall: 1.0, precision: 0.8873239436619719, f_beta: 0.9402985074626865\n",
      "train: step: 603, loss: 0.2246120423078537, acc: 0.9453125, recall: 0.9661016949152542, precision: 0.9193548387096774, f_beta: 0.9421487603305785\n",
      "train: step: 604, loss: 0.2063187062740326, acc: 0.9296875, recall: 0.9565217391304348, precision: 0.9166666666666666, f_beta: 0.9361702127659574\n",
      "train: step: 605, loss: 0.17264923453330994, acc: 0.921875, recall: 0.9154929577464789, precision: 0.9420289855072463, f_beta: 0.9285714285714286\n",
      "train: step: 606, loss: 0.20912893116474152, acc: 0.921875, recall: 0.9365079365079365, precision: 0.9076923076923077, f_beta: 0.9218749999999999\n",
      "train: step: 607, loss: 0.20270052552223206, acc: 0.9140625, recall: 0.9344262295081968, precision: 0.890625, f_beta: 0.9120000000000001\n",
      "train: step: 608, loss: 0.21515440940856934, acc: 0.921875, recall: 0.9393939393939394, precision: 0.9117647058823529, f_beta: 0.9253731343283583\n",
      "train: step: 609, loss: 0.2647329866886139, acc: 0.890625, recall: 0.8709677419354839, precision: 0.9, f_beta: 0.8852459016393444\n",
      "train: step: 610, loss: 0.17292146384716034, acc: 0.9375, recall: 0.9322033898305084, precision: 0.9322033898305084, f_beta: 0.9322033898305084\n",
      "train: step: 611, loss: 0.15888983011245728, acc: 0.9296875, recall: 0.9464285714285714, precision: 0.8983050847457628, f_beta: 0.9217391304347826\n",
      "train: step: 612, loss: 0.21061085164546967, acc: 0.8984375, recall: 0.8771929824561403, precision: 0.8928571428571429, f_beta: 0.8849557522123894\n",
      "train: step: 613, loss: 0.1538219004869461, acc: 0.9453125, recall: 0.9122807017543859, precision: 0.9629629629629629, f_beta: 0.9369369369369369\n",
      "train: step: 614, loss: 0.22530829906463623, acc: 0.8984375, recall: 0.8363636363636363, precision: 0.92, f_beta: 0.8761904761904761\n",
      "train: step: 615, loss: 0.17393521964550018, acc: 0.953125, recall: 0.9682539682539683, precision: 0.9384615384615385, f_beta: 0.953125\n",
      "train: step: 616, loss: 0.19139951467514038, acc: 0.9140625, recall: 0.9242424242424242, precision: 0.9104477611940298, f_beta: 0.9172932330827067\n",
      "train: step: 617, loss: 0.259928435087204, acc: 0.8984375, recall: 0.9206349206349206, precision: 0.8787878787878788, f_beta: 0.8992248062015504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 618, loss: 0.15197838842868805, acc: 0.9375, recall: 0.967741935483871, precision: 0.9090909090909091, f_beta: 0.9374999999999999\n",
      "train: step: 619, loss: 0.22449123859405518, acc: 0.90625, recall: 0.9102564102564102, precision: 0.9342105263157895, f_beta: 0.922077922077922\n",
      "train: step: 620, loss: 0.3180513083934784, acc: 0.8984375, recall: 0.921875, precision: 0.8805970149253731, f_beta: 0.9007633587786259\n",
      "train: step: 621, loss: 0.23186275362968445, acc: 0.921875, recall: 0.9180327868852459, precision: 0.9180327868852459, f_beta: 0.9180327868852459\n",
      "train: step: 622, loss: 0.18939067423343658, acc: 0.9375, recall: 0.9696969696969697, precision: 0.9142857142857143, f_beta: 0.9411764705882354\n",
      "train: step: 623, loss: 0.17832531034946442, acc: 0.9453125, recall: 0.9402985074626866, precision: 0.9545454545454546, f_beta: 0.9473684210526316\n",
      "train: step: 624, loss: 0.22526337206363678, acc: 0.9296875, recall: 0.9117647058823529, precision: 0.9538461538461539, f_beta: 0.9323308270676691\n",
      "start training model\n",
      "train: step: 625, loss: 0.1631207913160324, acc: 0.953125, recall: 0.9322033898305084, precision: 0.9649122807017544, f_beta: 0.9482758620689654\n",
      "train: step: 626, loss: 0.11557667702436447, acc: 0.96875, recall: 0.9848484848484849, precision: 0.9558823529411765, f_beta: 0.9701492537313432\n",
      "train: step: 627, loss: 0.15796016156673431, acc: 0.9296875, recall: 0.9523809523809523, precision: 0.9090909090909091, f_beta: 0.9302325581395349\n",
      "train: step: 628, loss: 0.12058216333389282, acc: 0.9609375, recall: 0.9365079365079365, precision: 0.9833333333333333, f_beta: 0.9593495934959351\n",
      "train: step: 629, loss: 0.1478593796491623, acc: 0.9296875, recall: 0.9210526315789473, precision: 0.958904109589041, f_beta: 0.9395973154362416\n",
      "train: step: 630, loss: 0.13575556874275208, acc: 0.9609375, recall: 0.96875, precision: 0.9538461538461539, f_beta: 0.9612403100775193\n",
      "train: step: 631, loss: 0.10776740312576294, acc: 0.9921875, recall: 1.0, precision: 0.9824561403508771, f_beta: 0.9911504424778761\n",
      "train: step: 632, loss: 0.10500121116638184, acc: 0.9921875, recall: 0.9807692307692307, precision: 1.0, f_beta: 0.9902912621359222\n",
      "train: step: 633, loss: 0.13483834266662598, acc: 0.9453125, recall: 0.9122807017543859, precision: 0.9629629629629629, f_beta: 0.9369369369369369\n",
      "train: step: 634, loss: 0.15260279178619385, acc: 0.9609375, recall: 0.9491525423728814, precision: 0.9655172413793104, f_beta: 0.9572649572649573\n",
      "train: step: 635, loss: 0.16514082252979279, acc: 0.953125, recall: 0.9523809523809523, precision: 0.9523809523809523, f_beta: 0.9523809523809523\n",
      "train: step: 636, loss: 0.18926700949668884, acc: 0.9375, recall: 0.9130434782608695, precision: 0.9692307692307692, f_beta: 0.9402985074626865\n",
      "train: step: 637, loss: 0.17895203828811646, acc: 0.953125, recall: 0.9594594594594594, precision: 0.9594594594594594, f_beta: 0.9594594594594594\n",
      "train: step: 638, loss: 0.14605900645256042, acc: 0.953125, recall: 0.9642857142857143, precision: 0.9310344827586207, f_beta: 0.9473684210526316\n",
      "train: step: 639, loss: 0.17641592025756836, acc: 0.9296875, recall: 0.984375, precision: 0.8873239436619719, f_beta: 0.9333333333333333\n",
      "train: step: 640, loss: 0.16808989644050598, acc: 0.921875, recall: 0.967741935483871, precision: 0.8823529411764706, f_beta: 0.923076923076923\n",
      "train: step: 641, loss: 0.13145750761032104, acc: 0.9453125, recall: 0.9655172413793104, precision: 0.9180327868852459, f_beta: 0.9411764705882353\n",
      "train: step: 642, loss: 0.14659692347049713, acc: 0.9296875, recall: 0.9230769230769231, precision: 0.9375, f_beta: 0.9302325581395349\n",
      "train: step: 643, loss: 0.15799999237060547, acc: 0.9609375, recall: 0.9558823529411765, precision: 0.9701492537313433, f_beta: 0.962962962962963\n",
      "train: step: 644, loss: 0.1555802971124649, acc: 0.9140625, recall: 0.8524590163934426, precision: 0.9629629629629629, f_beta: 0.9043478260869565\n",
      "train: step: 645, loss: 0.1487177312374115, acc: 0.9609375, recall: 0.9411764705882353, precision: 0.9846153846153847, f_beta: 0.962406015037594\n",
      "train: step: 646, loss: 0.14916744828224182, acc: 0.953125, recall: 0.9696969696969697, precision: 0.9411764705882353, f_beta: 0.955223880597015\n",
      "train: step: 647, loss: 0.14462542533874512, acc: 0.9609375, recall: 0.9354838709677419, precision: 0.9830508474576272, f_beta: 0.9586776859504132\n",
      "train: step: 648, loss: 0.15718109905719757, acc: 0.9375, recall: 0.9666666666666667, precision: 0.90625, f_beta: 0.9354838709677419\n",
      "train: step: 649, loss: 0.11715807020664215, acc: 0.96875, recall: 1.0, precision: 0.9365079365079365, f_beta: 0.9672131147540983\n",
      "train: step: 650, loss: 0.14009785652160645, acc: 0.9609375, recall: 0.9705882352941176, precision: 0.9565217391304348, f_beta: 0.9635036496350365\n",
      "train: step: 651, loss: 0.17160922288894653, acc: 0.9453125, recall: 0.9848484848484849, precision: 0.9154929577464789, f_beta: 0.948905109489051\n",
      "train: step: 652, loss: 0.18178510665893555, acc: 0.921875, recall: 0.9180327868852459, precision: 0.9180327868852459, f_beta: 0.9180327868852459\n",
      "train: step: 653, loss: 0.15772108733654022, acc: 0.9609375, recall: 0.9672131147540983, precision: 0.9516129032258065, f_beta: 0.959349593495935\n",
      "train: step: 654, loss: 0.20053833723068237, acc: 0.9296875, recall: 0.9310344827586207, precision: 0.9152542372881356, f_beta: 0.923076923076923\n",
      "train: step: 655, loss: 0.14885523915290833, acc: 0.9453125, recall: 0.921875, precision: 0.9672131147540983, f_beta: 0.944\n",
      "train: step: 656, loss: 0.17670020461082458, acc: 0.9375, recall: 0.8769230769230769, precision: 1.0, f_beta: 0.9344262295081968\n",
      "train: step: 657, loss: 0.13949179649353027, acc: 0.9453125, recall: 0.9428571428571428, precision: 0.9565217391304348, f_beta: 0.9496402877697843\n",
      "train: step: 658, loss: 0.2529599964618683, acc: 0.8984375, recall: 0.8947368421052632, precision: 0.8793103448275862, f_beta: 0.8869565217391304\n",
      "train: step: 659, loss: 0.17755720019340515, acc: 0.9296875, recall: 0.9166666666666666, precision: 0.9565217391304348, f_beta: 0.9361702127659574\n",
      "train: step: 660, loss: 0.17471225559711456, acc: 0.9296875, recall: 0.9384615384615385, precision: 0.9242424242424242, f_beta: 0.9312977099236641\n",
      "train: step: 661, loss: 0.13993553817272186, acc: 0.953125, recall: 0.9852941176470589, precision: 0.9305555555555556, f_beta: 0.9571428571428572\n",
      "train: step: 662, loss: 0.12751296162605286, acc: 0.96875, recall: 0.9833333333333333, precision: 0.9516129032258065, f_beta: 0.9672131147540983\n",
      "train: step: 663, loss: 0.1653454750776291, acc: 0.953125, recall: 0.9722222222222222, precision: 0.9459459459459459, f_beta: 0.9589041095890412\n",
      "train: step: 664, loss: 0.17576801776885986, acc: 0.9296875, recall: 0.9655172413793104, precision: 0.8888888888888888, f_beta: 0.9256198347107438\n",
      "train: step: 665, loss: 0.16069820523262024, acc: 0.953125, recall: 0.9701492537313433, precision: 0.9420289855072463, f_beta: 0.9558823529411764\n",
      "train: step: 666, loss: 0.18768012523651123, acc: 0.9296875, recall: 0.8888888888888888, precision: 0.9655172413793104, f_beta: 0.9256198347107438\n",
      "train: step: 667, loss: 0.17411178350448608, acc: 0.921875, recall: 0.881578947368421, precision: 0.9852941176470589, f_beta: 0.9305555555555556\n",
      "train: step: 668, loss: 0.17410819232463837, acc: 0.9296875, recall: 0.8888888888888888, precision: 0.9655172413793104, f_beta: 0.9256198347107438\n",
      "train: step: 669, loss: 0.15461242198944092, acc: 0.9453125, recall: 0.921875, precision: 0.9672131147540983, f_beta: 0.944\n",
      "train: step: 670, loss: 0.13415120542049408, acc: 0.953125, recall: 0.9692307692307692, precision: 0.9402985074626866, f_beta: 0.9545454545454547\n",
      "train: step: 671, loss: 0.1597062647342682, acc: 0.921875, recall: 0.9838709677419355, precision: 0.8714285714285714, f_beta: 0.9242424242424242\n",
      "train: step: 672, loss: 0.13796372711658478, acc: 0.953125, recall: 0.9836065573770492, precision: 0.9230769230769231, f_beta: 0.9523809523809524\n",
      "train: step: 673, loss: 0.12391059100627899, acc: 0.9609375, recall: 0.9523809523809523, precision: 0.967741935483871, f_beta: 0.96\n",
      "train: step: 674, loss: 0.1302943229675293, acc: 0.953125, recall: 0.9411764705882353, precision: 0.9411764705882353, f_beta: 0.9411764705882353\n",
      "train: step: 675, loss: 0.16000637412071228, acc: 0.96875, recall: 0.95, precision: 0.9827586206896551, f_beta: 0.9661016949152542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 676, loss: 0.1380249708890915, acc: 0.9765625, recall: 0.953125, precision: 1.0, f_beta: 0.976\n",
      "train: step: 677, loss: 0.18933534622192383, acc: 0.9296875, recall: 0.9827586206896551, precision: 0.8769230769230769, f_beta: 0.9268292682926829\n",
      "train: step: 678, loss: 0.09839662909507751, acc: 0.9765625, recall: 0.9841269841269841, precision: 0.96875, f_beta: 0.9763779527559054\n",
      "train: step: 679, loss: 0.15274228155612946, acc: 0.953125, recall: 0.9545454545454546, precision: 0.9545454545454546, f_beta: 0.9545454545454546\n",
      "train: step: 680, loss: 0.16599339246749878, acc: 0.953125, recall: 0.953125, precision: 0.953125, f_beta: 0.953125\n",
      "train: step: 681, loss: 0.17996302247047424, acc: 0.9375, recall: 0.9333333333333333, precision: 0.9333333333333333, f_beta: 0.9333333333333333\n",
      "train: step: 682, loss: 0.12773776054382324, acc: 0.9609375, recall: 0.9841269841269841, precision: 0.9393939393939394, f_beta: 0.9612403100775193\n",
      "train: step: 683, loss: 0.19575637578964233, acc: 0.921875, recall: 0.9090909090909091, precision: 0.9090909090909091, f_beta: 0.9090909090909091\n",
      "train: step: 684, loss: 0.14824725687503815, acc: 0.984375, recall: 0.9710144927536232, precision: 1.0, f_beta: 0.9852941176470589\n",
      "train: step: 685, loss: 0.15883927047252655, acc: 0.9296875, recall: 0.9047619047619048, precision: 0.95, f_beta: 0.9268292682926829\n",
      "train: step: 686, loss: 0.11765077710151672, acc: 0.9609375, recall: 0.9464285714285714, precision: 0.9636363636363636, f_beta: 0.9549549549549549\n",
      "train: step: 687, loss: 0.12726323306560516, acc: 0.9765625, recall: 0.9682539682539683, precision: 0.9838709677419355, f_beta: 0.976\n",
      "train: step: 688, loss: 0.22440144419670105, acc: 0.9140625, recall: 0.9402985074626866, precision: 0.9, f_beta: 0.9197080291970803\n",
      "train: step: 689, loss: 0.1320447474718094, acc: 0.9609375, recall: 0.9705882352941176, precision: 0.9565217391304348, f_beta: 0.9635036496350365\n",
      "train: step: 690, loss: 0.15036974847316742, acc: 0.953125, recall: 0.9259259259259259, precision: 0.9615384615384616, f_beta: 0.9433962264150944\n",
      "train: step: 691, loss: 0.09837169200181961, acc: 0.984375, recall: 1.0, precision: 0.9682539682539683, f_beta: 0.9838709677419354\n",
      "train: step: 692, loss: 0.15695257484912872, acc: 0.9375, recall: 0.90625, precision: 0.9666666666666667, f_beta: 0.9354838709677419\n",
      "train: step: 693, loss: 0.11945660412311554, acc: 0.96875, recall: 0.9310344827586207, precision: 1.0, f_beta: 0.9642857142857143\n",
      "train: step: 694, loss: 0.15758608281612396, acc: 0.9375, recall: 0.9516129032258065, precision: 0.921875, f_beta: 0.9365079365079365\n",
      "train: step: 695, loss: 0.11352964490652084, acc: 0.96875, recall: 1.0, precision: 0.9487179487179487, f_beta: 0.9736842105263158\n",
      "train: step: 696, loss: 0.12543180584907532, acc: 0.9609375, recall: 1.0, precision: 0.9315068493150684, f_beta: 0.9645390070921985\n",
      "train: step: 697, loss: 0.14967139065265656, acc: 0.9375, recall: 0.921875, precision: 0.9516129032258065, f_beta: 0.9365079365079365\n",
      "train: step: 698, loss: 0.1665187031030655, acc: 0.9453125, recall: 1.0, precision: 0.8939393939393939, f_beta: 0.944\n",
      "train: step: 699, loss: 0.14559856057167053, acc: 0.9375, recall: 0.9420289855072463, precision: 0.9420289855072463, f_beta: 0.9420289855072463\n",
      "train: step: 700, loss: 0.12385113537311554, acc: 0.9609375, recall: 0.9253731343283582, precision: 1.0, f_beta: 0.9612403100775194\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:36:20.899735, step: 700, loss: 0.32430119048326445, acc: 0.8647836538461539,precision: 0.8344582751872733, recall: 0.893003047230169, f_beta: 0.8614737918741944\n",
      "Saved model checkpoint to ../model/textCNN/model/my-model-700\n",
      "\n",
      "train: step: 701, loss: 0.1324683129787445, acc: 0.9453125, recall: 0.9344262295081968, precision: 0.95, f_beta: 0.9421487603305784\n",
      "train: step: 702, loss: 0.1462947279214859, acc: 0.9375, recall: 0.9344262295081968, precision: 0.9344262295081968, f_beta: 0.9344262295081968\n",
      "train: step: 703, loss: 0.1319936215877533, acc: 0.9453125, recall: 0.9090909090909091, precision: 0.9836065573770492, f_beta: 0.9448818897637795\n",
      "train: step: 704, loss: 0.13326996564865112, acc: 0.96875, recall: 0.9622641509433962, precision: 0.9622641509433962, f_beta: 0.9622641509433962\n",
      "train: step: 705, loss: 0.14204901456832886, acc: 0.9765625, recall: 0.9710144927536232, precision: 0.9852941176470589, f_beta: 0.9781021897810219\n",
      "train: step: 706, loss: 0.20043659210205078, acc: 0.90625, recall: 0.9482758620689655, precision: 0.859375, f_beta: 0.9016393442622951\n",
      "train: step: 707, loss: 0.15567663311958313, acc: 0.953125, recall: 0.9701492537313433, precision: 0.9420289855072463, f_beta: 0.9558823529411764\n",
      "train: step: 708, loss: 0.1532537192106247, acc: 0.9296875, recall: 1.0, precision: 0.8524590163934426, f_beta: 0.9203539823008848\n",
      "train: step: 709, loss: 0.1341187208890915, acc: 0.9609375, recall: 0.984375, precision: 0.9402985074626866, f_beta: 0.9618320610687023\n",
      "train: step: 710, loss: 0.13202908635139465, acc: 0.9453125, recall: 0.9295774647887324, precision: 0.9705882352941176, f_beta: 0.9496402877697842\n",
      "train: step: 711, loss: 0.15261581540107727, acc: 0.9375, recall: 0.9523809523809523, precision: 0.9230769230769231, f_beta: 0.9375\n",
      "train: step: 712, loss: 0.1765773594379425, acc: 0.9296875, recall: 0.8787878787878788, precision: 0.9830508474576272, f_beta: 0.9279999999999999\n",
      "train: step: 713, loss: 0.17219914495944977, acc: 0.9375, recall: 0.9206349206349206, precision: 0.9508196721311475, f_beta: 0.9354838709677418\n",
      "train: step: 714, loss: 0.10297077149152756, acc: 0.984375, recall: 0.9841269841269841, precision: 0.9841269841269841, f_beta: 0.9841269841269841\n",
      "train: step: 715, loss: 0.15179719030857086, acc: 0.9453125, recall: 0.953125, precision: 0.9384615384615385, f_beta: 0.9457364341085271\n",
      "train: step: 716, loss: 0.21878090500831604, acc: 0.9453125, recall: 0.9696969696969697, precision: 0.927536231884058, f_beta: 0.9481481481481481\n",
      "train: step: 717, loss: 0.15751072764396667, acc: 0.9453125, recall: 0.9811320754716981, precision: 0.896551724137931, f_beta: 0.9369369369369369\n",
      "train: step: 718, loss: 0.17949964106082916, acc: 0.9375, recall: 0.9137931034482759, precision: 0.9464285714285714, f_beta: 0.9298245614035087\n",
      "train: step: 719, loss: 0.19771315157413483, acc: 0.90625, recall: 0.9016393442622951, precision: 0.9016393442622951, f_beta: 0.9016393442622952\n",
      "train: step: 720, loss: 0.20968234539031982, acc: 0.921875, recall: 0.9491525423728814, precision: 0.8888888888888888, f_beta: 0.9180327868852458\n",
      "train: step: 721, loss: 0.1622471958398819, acc: 0.953125, recall: 0.9393939393939394, precision: 0.96875, f_beta: 0.9538461538461539\n",
      "train: step: 722, loss: 0.17190049588680267, acc: 0.9296875, recall: 0.8833333333333333, precision: 0.9636363636363636, f_beta: 0.9217391304347826\n",
      "train: step: 723, loss: 0.17250284552574158, acc: 0.921875, recall: 0.8769230769230769, precision: 0.9661016949152542, f_beta: 0.9193548387096773\n",
      "train: step: 724, loss: 0.11818509548902512, acc: 0.9609375, recall: 0.9433962264150944, precision: 0.9615384615384616, f_beta: 0.9523809523809524\n",
      "train: step: 725, loss: 0.16723614931106567, acc: 0.9609375, recall: 0.9436619718309859, precision: 0.9852941176470589, f_beta: 0.9640287769784172\n",
      "train: step: 726, loss: 0.1295025497674942, acc: 0.9609375, recall: 0.9821428571428571, precision: 0.9322033898305084, f_beta: 0.9565217391304348\n",
      "train: step: 727, loss: 0.19133003056049347, acc: 0.921875, recall: 0.9393939393939394, precision: 0.9117647058823529, f_beta: 0.9253731343283583\n",
      "train: step: 728, loss: 0.13905373215675354, acc: 0.96875, recall: 0.987012987012987, precision: 0.9620253164556962, f_beta: 0.9743589743589742\n",
      "train: step: 729, loss: 0.18073004484176636, acc: 0.8984375, recall: 0.9848484848484849, precision: 0.8441558441558441, f_beta: 0.9090909090909091\n",
      "train: step: 730, loss: 0.18125668168067932, acc: 0.9453125, recall: 0.9354838709677419, precision: 0.9508196721311475, f_beta: 0.943089430894309\n",
      "train: step: 731, loss: 0.1371370404958725, acc: 0.9453125, recall: 0.953125, precision: 0.9384615384615385, f_beta: 0.9457364341085271\n",
      "train: step: 732, loss: 0.16768327355384827, acc: 0.953125, recall: 0.9545454545454546, precision: 0.9545454545454546, f_beta: 0.9545454545454546\n",
      "train: step: 733, loss: 0.1547432690858841, acc: 0.9296875, recall: 0.8666666666666667, precision: 0.9811320754716981, f_beta: 0.9203539823008849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 734, loss: 0.14827890694141388, acc: 0.953125, recall: 0.9375, precision: 0.967741935483871, f_beta: 0.9523809523809523\n",
      "train: step: 735, loss: 0.180472731590271, acc: 0.9140625, recall: 0.8571428571428571, precision: 0.9411764705882353, f_beta: 0.897196261682243\n",
      "train: step: 736, loss: 0.1393282264471054, acc: 0.96875, recall: 0.9594594594594594, precision: 0.9861111111111112, f_beta: 0.9726027397260274\n",
      "train: step: 737, loss: 0.1242164820432663, acc: 0.953125, recall: 0.984375, precision: 0.9264705882352942, f_beta: 0.9545454545454545\n",
      "train: step: 738, loss: 0.1491786241531372, acc: 0.9375, recall: 0.9833333333333333, precision: 0.8939393939393939, f_beta: 0.9365079365079364\n",
      "train: step: 739, loss: 0.11156563460826874, acc: 0.9765625, recall: 0.9855072463768116, precision: 0.9714285714285714, f_beta: 0.9784172661870504\n",
      "train: step: 740, loss: 0.16168689727783203, acc: 0.9453125, recall: 0.9830508474576272, precision: 0.90625, f_beta: 0.943089430894309\n",
      "train: step: 741, loss: 0.14518052339553833, acc: 0.96875, recall: 0.9841269841269841, precision: 0.9538461538461539, f_beta: 0.96875\n",
      "train: step: 742, loss: 0.15155306458473206, acc: 0.9453125, recall: 0.9310344827586207, precision: 0.9473684210526315, f_beta: 0.9391304347826087\n",
      "train: step: 743, loss: 0.19886982440948486, acc: 0.921875, recall: 0.9076923076923077, precision: 0.9365079365079365, f_beta: 0.9218749999999999\n",
      "train: step: 744, loss: 0.18598799407482147, acc: 0.921875, recall: 0.9206349206349206, precision: 0.9206349206349206, f_beta: 0.9206349206349206\n",
      "train: step: 745, loss: 0.135225310921669, acc: 0.953125, recall: 0.9344262295081968, precision: 0.9661016949152542, f_beta: 0.95\n",
      "train: step: 746, loss: 0.1451670527458191, acc: 0.9453125, recall: 0.92, precision: 0.9857142857142858, f_beta: 0.9517241379310346\n",
      "train: step: 747, loss: 0.18757730722427368, acc: 0.921875, recall: 0.8656716417910447, precision: 0.9830508474576272, f_beta: 0.9206349206349207\n",
      "train: step: 748, loss: 0.1399591863155365, acc: 0.9609375, recall: 0.9649122807017544, precision: 0.9482758620689655, f_beta: 0.9565217391304347\n",
      "train: step: 749, loss: 0.15608254075050354, acc: 0.9296875, recall: 0.9242424242424242, precision: 0.9384615384615385, f_beta: 0.9312977099236641\n",
      "train: step: 750, loss: 0.1512591540813446, acc: 0.9453125, recall: 1.0, precision: 0.9, f_beta: 0.9473684210526316\n",
      "train: step: 751, loss: 0.13209697604179382, acc: 0.9765625, recall: 1.0, precision: 0.9565217391304348, f_beta: 0.9777777777777777\n",
      "train: step: 752, loss: 0.21216624975204468, acc: 0.9296875, recall: 0.9836065573770492, precision: 0.8823529411764706, f_beta: 0.9302325581395349\n",
      "train: step: 753, loss: 0.16419930756092072, acc: 0.9375, recall: 0.9459459459459459, precision: 0.9459459459459459, f_beta: 0.9459459459459459\n",
      "train: step: 754, loss: 0.17434751987457275, acc: 0.9296875, recall: 0.9154929577464789, precision: 0.9558823529411765, f_beta: 0.9352517985611511\n",
      "train: step: 755, loss: 0.16613218188285828, acc: 0.96875, recall: 0.9402985074626866, precision: 1.0, f_beta: 0.9692307692307692\n",
      "train: step: 756, loss: 0.2003609538078308, acc: 0.953125, recall: 0.9384615384615385, precision: 0.9682539682539683, f_beta: 0.953125\n",
      "train: step: 757, loss: 0.13545659184455872, acc: 0.9609375, recall: 0.9552238805970149, precision: 0.9696969696969697, f_beta: 0.9624060150375939\n",
      "train: step: 758, loss: 0.15032468736171722, acc: 0.953125, recall: 0.9459459459459459, precision: 0.9722222222222222, f_beta: 0.9589041095890412\n",
      "train: step: 759, loss: 0.1457395851612091, acc: 0.96875, recall: 1.0, precision: 0.9493670886075949, f_beta: 0.974025974025974\n",
      "train: step: 760, loss: 0.18701796233654022, acc: 0.953125, recall: 0.9354838709677419, precision: 0.9666666666666667, f_beta: 0.9508196721311476\n",
      "train: step: 761, loss: 0.14309465885162354, acc: 0.9609375, recall: 0.9565217391304348, precision: 0.9705882352941176, f_beta: 0.9635036496350365\n",
      "train: step: 762, loss: 0.1503835767507553, acc: 0.9140625, recall: 0.9714285714285714, precision: 0.8831168831168831, f_beta: 0.9251700680272108\n",
      "train: step: 763, loss: 0.21238984167575836, acc: 0.9375, recall: 1.0, precision: 0.8961038961038961, f_beta: 0.9452054794520548\n",
      "train: step: 764, loss: 0.12942248582839966, acc: 0.9453125, recall: 0.984375, precision: 0.9130434782608695, f_beta: 0.9473684210526315\n",
      "train: step: 765, loss: 0.14331889152526855, acc: 0.9609375, recall: 0.9696969696969697, precision: 0.9552238805970149, f_beta: 0.9624060150375939\n",
      "train: step: 766, loss: 0.16827496886253357, acc: 0.9375, recall: 0.9322033898305084, precision: 0.9322033898305084, f_beta: 0.9322033898305084\n",
      "train: step: 767, loss: 0.1643660068511963, acc: 0.90625, recall: 0.8301886792452831, precision: 0.9361702127659575, f_beta: 0.88\n",
      "train: step: 768, loss: 0.16157662868499756, acc: 0.953125, recall: 0.9047619047619048, precision: 1.0, f_beta: 0.9500000000000001\n",
      "train: step: 769, loss: 0.20713011920452118, acc: 0.9375, recall: 0.8709677419354839, precision: 1.0, f_beta: 0.9310344827586207\n",
      "train: step: 770, loss: 0.09069538861513138, acc: 0.9609375, recall: 0.9333333333333333, precision: 0.9824561403508771, f_beta: 0.9572649572649572\n",
      "train: step: 771, loss: 0.1779036521911621, acc: 0.9296875, recall: 0.971830985915493, precision: 0.9078947368421053, f_beta: 0.9387755102040817\n",
      "train: step: 772, loss: 0.22951075434684753, acc: 0.921875, recall: 0.9230769230769231, precision: 0.9230769230769231, f_beta: 0.9230769230769231\n",
      "train: step: 773, loss: 0.15968988835811615, acc: 0.9375, recall: 0.9577464788732394, precision: 0.9315068493150684, f_beta: 0.9444444444444444\n",
      "train: step: 774, loss: 0.15181955695152283, acc: 0.921875, recall: 0.9692307692307692, precision: 0.8873239436619719, f_beta: 0.9264705882352942\n",
      "train: step: 775, loss: 0.15713074803352356, acc: 0.953125, recall: 0.953125, precision: 0.953125, f_beta: 0.953125\n",
      "train: step: 776, loss: 0.13054929673671722, acc: 0.953125, recall: 0.9726027397260274, precision: 0.9466666666666667, f_beta: 0.9594594594594594\n",
      "train: step: 777, loss: 0.09435100108385086, acc: 0.984375, recall: 1.0, precision: 0.9714285714285714, f_beta: 0.9855072463768115\n",
      "train: step: 778, loss: 0.16631576418876648, acc: 0.9453125, recall: 0.9305555555555556, precision: 0.9710144927536232, f_beta: 0.9503546099290779\n",
      "train: step: 779, loss: 0.18891339004039764, acc: 0.9375, recall: 0.9558823529411765, precision: 0.9285714285714286, f_beta: 0.9420289855072465\n",
      "train: step: 780, loss: 0.1269279569387436, acc: 0.9453125, recall: 0.9122807017543859, precision: 0.9629629629629629, f_beta: 0.9369369369369369\n",
      "WARNING:tensorflow:From <ipython-input-26-add0eab155e7>:158: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ../model/textCNN/savedModel\\saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "labelList = data.labelList\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(cnn.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        savedModelPath = \"../model/textCNN/savedModel\"\n",
    "        if os.path.exists(savedModelPath):\n",
    "            os.rmdir(savedModelPath)\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(savedModelPath)\n",
    "            \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              cnn.inputX: batchX,\n",
    "              cnn.inputY: batchY,\n",
    "              cnn.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, cnn.loss, cnn.predictions],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "\n",
    "                \n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY,\n",
    "                                                              labels=labelList)\n",
    "                \n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, prec, recall, f_beta\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.inputX: batchX,\n",
    "              cnn.inputY: batchY,\n",
    "              cnn.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions = sess.run(\n",
    "                [summaryOp, globalStep, cnn.loss, cnn.predictions],\n",
    "                feed_dict)\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "            \n",
    "                acc, precision, recall, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, precision, recall, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, precision, recall, f_beta\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                loss, acc, prec, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
    "                \n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                print(\"train: step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
    "                    currentStep, loss, acc, recall, prec, f_beta))\n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    f_betas = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, precision, recall, f_beta = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        f_betas.append(f_beta)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {},precision: {}, recall: {}, f_beta: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(precisions),\n",
    "                                                                                                       mean(recalls), mean(f_betas)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/textCNN/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(cnn.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(cnn.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"predictions\": tf.saved_model.utils.build_tensor_info(cnn.predictions)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/textCNN/model/my-model-700\n",
      "['1']\n"
     ]
    }
   ],
   "source": [
    "x = \"this movie is full of references like mad max ii the wild one and many others the ladybug´s face it´s a clear reference or tribute to peter lorre this movie is a masterpiece we´ll talk much more about in the future\"\n",
    "\n",
    "# 注：下面两个词典要保证和当前加载的模型对应的词典是一致的\n",
    "with open(\"../data/wordJson/word2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word2idx = json.load(f)\n",
    "        \n",
    "with open(\"../data/wordJson/label2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label2idx = json.load(f)\n",
    "idx2label = {value: key for key, value in label2idx.items()}\n",
    "    \n",
    "xIds = [word2idx.get(item, word2idx[\"UNK\"]) for item in x.split(\" \")]\n",
    "if len(xIds) >= config.sequenceLength:\n",
    "    xIds = xIds[:config.sequenceLength]\n",
    "else:\n",
    "    xIds = xIds + [word2idx[\"PAD\"]] * (config.sequenceLength - len(xIds))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False, gpu_options=gpu_options)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    with sess.as_default():\n",
    "        checkpoint_file = tf.train.latest_checkpoint(\"../model/textCNN/model/\")\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # 获得需要喂给模型的参数，输出的结果依赖的输入值\n",
    "        inputX = graph.get_operation_by_name(\"inputX\").outputs[0]\n",
    "        dropoutKeepProb = graph.get_operation_by_name(\"dropoutKeepProb\").outputs[0]\n",
    "\n",
    "        # 获得输出的结果\n",
    "        predictions = graph.get_tensor_by_name(\"output/predictions:0\")\n",
    "\n",
    "        pred = sess.run(predictions, feed_dict={inputX: [xIds], dropoutKeepProb: 1.0})[0]\n",
    "        \n",
    "pred = [idx2label[item] for item in pred]     \n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
